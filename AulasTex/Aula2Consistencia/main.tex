
\documentclass[12pt]{beamer}

%\input{C:/Users/Fernando/Documents/Ensino/DET/PosGraduacao/EST711/Configuracoes/layout}
\input{../Configuracoes/layout}

\title{Inferência Estatística II}
\author{Prof. Fernando de Souza Bastos\texorpdfstring{\\ fernando.bastos@ufv.br}{}}
\institute{Departamento de Estatística\texorpdfstring{\\ Programa de Pós-Graduação em Estatística Aplicada e Biometria}\texorpdfstring{\\ Universidade Federal de Viçosa}{}\texorpdfstring{\\ Campus UFV - Viçosa}{}}
\date{}
\newcommand\mytext{Aula 2}
\newcommand\mytextt{Fernando de Souza Bastos}
\newcommand\mytexttt{\url{https://est711.github.io/}}


\begin{document}
%\SweaveOpts{concordance=TRUE}

\frame{\titlepage}

\begin{frame}{}
\frametitle{\bf Sumário}
\tableofcontents
\end{frame}

\section{Consistência}
\begin{frame}{Consistência}
%\begin{block}{}
\begin{definicao}
\justifying
 Seja $X$ uma variável aleatória com função de distribuição acumulada $F(x, \theta)$, $\theta \in \mathcal{A}\subseteq \Omega$. Seja $X_1, \ldots, X_{n}$ uma amostra da distribuição de $X$ e seja $T_{n}$ uma estatística $(T_{n}=T(X_1, \ldots, X_{n}))$. Dizemos que $T_{n}$ é um estimador consistente para $\theta$ se $T_{n} \xrightarrow{P} \theta$.
\end{definicao}    
%\end{block}
\end{frame}

\begin{frame}{Exemplo}
\begin{block}{}
\justifying
Sejam $X_{1}, \ldots, X_{n},\ldots$ uma sequência de variáveis aleatórias iid de uma distribuição com média finita $\mu$ e variância $\sigma^{2}<+\infty$, então, pela Lei Fraca dos Grandes Números, temos que, $\Bar{X_{n}}=\dfrac{\Sumi X_{i}}{n}\ConvP \mu.$ Ou seja, $\Bar{X_{n}}$ é um estimador consistente de $\mu$.
\end{block}
\end{frame}

\begin{frame}{Exemplo}
\begin{block}{}
\justifying
Sejam $X_1, \ldots, X_{n}$ uma amostra aleatória de uma distribuição com média $\mu$ e variância $\sigma^{2}<+\infty$. Suponha que $E[X^{4}_{1}] < +\infty$, de tal forma que $Var(S^{2}) < +\infty$. 
\begin{align*}
S^2_{n} &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_{n})^2 \\
&=\frac{1}{n-1} \left(\sum_{i=1}^n X_{i}^{2} - n\overline{X}_{n}^{2}\right)\\
&= \frac{n}{n-1} \left(\frac{1}{n}\sum_{i=1}^n X^2_i - \overline{X}^{2}_{n}\right) \\
&\xrightarrow{P} 1 \cdot [E(X^2_1) - \mu^2] = \sigma^2.
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Portanto, a variância da amostra é um estimador consistente de $\sigma^2$. A partir da discussão acima, temos imediatamente que $S_{n} \xrightarrow{P} \sigma$; ou seja, o desvio padrão da amostra é um estimador consistente do desvio padrão populacional. Vejam estes exemplos anteriores na prática, \href{https://est711.shinyapps.io/ConvergenciaProbabilidade/}{Clique aqui!}.
\end{block}
\end{frame}

\begin{frame}{Exemplo}
\vspace{-0.3cm}
\begin{block}{}
\justifying
Considere $X_{i}\overset{\text{iid}}{\sim} U(0,\theta),~i=1,2,\ldots,n,$ e $Y_{n}=max\{X_{1}, \ldots, X_{n}\}.$ Seja $\varepsilon>0,$ segue que:
\begin{align*}
    P(|Y_{n}-\theta|\geq \varepsilon)&=P(\theta-Y_{n}\geq \varepsilon)\\
    &=P(Y_{n}\leq \theta-\varepsilon).
\end{align*}
\end{block}
\pause
\begin{block}{}
\justifying
Se $\theta-\varepsilon\leq 0,$ então $P(Y_{n}\leq \theta-\varepsilon)=0,$ pois $0\leq Y_{n}\leq \theta,$ com $P(0\leq Y_{n}\leq \theta)=1.$
\end{block}
\pause
\begin{block}{}
\justifying
Se $0<\varepsilon<\theta$ então,
\begin{align*}
P(|Y_{n}-\theta|\geq \varepsilon)&=P(Y_{n}\leq \theta- \varepsilon)=F_{Y_{n}}(\theta-\varepsilon)=\Big(\dfrac{\theta-\varepsilon}{\theta}\Big)^{n}\xrightarrow[n\rightarrow\infty]{} 0
\end{align*}
Ou seja, $Y_{n} \xrightarrow{P} \theta.$ Logo, $Y_{n}$ é um estimador consistente para $\theta.$
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{\Home}
\justifying
Exercícios 2.8.18, 5.1.2, 5.1.3, 5.1.7 e 5.1.9
\end{block}
\end{frame}

\section{Convergência em Distribuição}
\begin{frame}{Convergência em Distribuição}
\begin{definicao}
\justifying
Seja $\{X_{n}\}_{n\geq 1}$ uma sequência de variáveis aleatórias com função de distribuição $F_{X_{n}},~n\geq 1.$ Seja $X$ uma variável aleatória com função de distribuição $F_{X}.$ Seja $C(F_{X})$ o conjunto de todos os pontos de continuidade de $F_{X}$. Dizemos que $X_{n}$ converge em distribuição para $X$ se,
$$\lim_{{n \to \infty}} F_{X_{n}}(x) = F_X(x), ~\forall x \in C(F_X).$$ Denotamos essa convergência por $X_{n} \overset{D}{\rightarrow} X$ ou $X_{n} \overset{\mathcal{L}}{\rightarrow} X.$
\end{definicao}
\pause
\begin{block}{}
	À medida que o tamanho da amostra aumenta, a distribuição das médias amostrais se aproxima da distribuição normal, veja isso acontecendo na prática, \href{https://est711.shinyapps.io/ConvergenciaProbabilidade/}{Clique aqui!}.
\end{block}

\end{frame}

\begin{frame}{Exemplo}
\begin{block}{}
\justifying
\begin{align*}
    P\Big(X_{n}=\dfrac{1}{n}\Big)=1, \forall n\geq 1,~P(X=0)=1, \mathcal{C}(F_{X}(x))=\{x\in\R; x\neq 0\}
\end{align*}
\end{block}
\begin{block}{}
\begin{figure}%
    \centering
    \subfloat[\centering $P\Big(X_{n}=\dfrac{1}{n}\Big)=1$]{{\begin{tikzpicture}[scale=0.75]
%plano cartesiano
\draw[->] (-2,0) -- (3,0) node[right]{$x$};
\draw[->] (0,-.5) -- (0,2) node[above]{$F_{X_{n}}(x)$};
%Fun ̧c~ao
\draw[line width=2pt,domain=-2:0.96] plot
(\x,0);
\draw[fill=white] (1,0) circle (2pt);
\fill (1,1) circle (2pt);
\draw[line width=1pt,domain=1:2.5] plot
(\x,1);
% ́Indices
\draw[dashed] (1,1) -- (1,0) node[below,scale=.7] {$\dfrac{1}{n}$};
\node[scale=.7] at (-0.2,1) {$1$};
\end{tikzpicture} }}%
    \qquad
    \subfloat[\centering $P(X=0)=1$]{{\begin{tikzpicture}[scale=0.75]
%plano cartesiano
\draw[->] (-2,0) -- (3,0) node[right]{$x$};
\draw[->] (0,-.5) -- (0,2) node[above]{$F_{X}(x)$};
%Fun ̧c~ao
\draw[line width=2pt,domain=-2:-0.04] plot
(\x,0);
\draw[fill=white] (0,0) circle (2pt);
\fill (0,1) circle (2pt);
\draw[line width=1pt,domain=0:2.5] plot
(\x,1);
% ́Indices
\draw[dashed] (0,1) -- (0,0) node[below,scale=.7] {$0$};
\node[scale=.7] at (-0.2,1) {$1$};
\end{tikzpicture}}}%
    \caption{${\displaystyle \lim_{n\rightarrow+\infty}}F_{X_{n}}(x)=F_{X}(x),~\forall x\neq 0,~\text{ou seja}~X_{n} \overset{D}{\rightarrow} X$}%
    \label{fig:example}%
\end{figure}
\end{block}
\end{frame}

\begin{frame}{Exemplo 2 (Convergência em Distribuição não Implica Convergência em Probabilidade)}
\begin{block}{}
\justifying
Seja $X$ uma variável aleatória contínua simétrica em torno do zero (ou seja, se $f$ denota sua densidade, então $f(x) = f(-x), \forall x\in \R$. Neste caso, $X$ e $-X$ tem a mesma distribuição (Verifiquem!). Defina a sequência de variáveis aleatórias $X_{n}$ como:
$X_{n} = \begin{cases} 
X, & \text{se $n$ é par} \\ 
-X, & \text{se $n$ é impar} 
\end{cases}$
\end{block}
\pause
\begin{block}{}
\justifying
É fácil ver que $F_{X_{n}}(x)=F_{X}(x).$ Logo, $X_{n} \overset{D}{\rightarrow} X.$ Porém, $X_{n} \overset{P}{\cancel{\rightarrow}} X,$ pois 
$P(|X_{n}-X|\geq\varepsilon) = \begin{cases} 
0, & \text{se $n$ é par} \\ 
P(2|X|\geq \varepsilon), & \text{se $n$ é impar}
\end{cases}$
\end{block}
\end{frame}

\begin{frame}{Exemplo 3}
\begin{block}{}
\justifying
Seja $T_{n}$ uma variável aleatória com distribuição t-Student com $n$ graus de liberdade, ou seja, a densidade de $T_{n}$ é dada por:
\begin{align*}
    f_{T_{n}}(y)=\dfrac{\Gamma\Big(\dfrac{n+1}{2}\Big)}{\sqrt{n\pi}\Gamma\Big(\dfrac{n}{2}\Big)}\dfrac{1}{\Big(1+\dfrac{y^{2}}{n}\Big)^{\frac{n+1}{2}}},~y\in \R
\end{align*}
\end{block}
\pause
\begin{block}{}
\justifying
Temos que,
\begin{align*}
\lim_{n\rightarrow+\infty}F_{T_{n}}(t)=\lim_{n\rightarrow+\infty}\int_{-\infty}^{t}\dfrac{\Gamma\Big(\dfrac{n+1}{2}\Big)}{\sqrt{n\pi}\Gamma\Big(\dfrac{n}{2}\Big)}\dfrac{1}{\Big(1+\dfrac{y^{2}}{n}\Big)^{\frac{n+1}{2}}}dy
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\begin{align*}
\lim_{n\rightarrow+\infty}F_{T_{n}}(t)&=\lim_{n\rightarrow+\infty}\int_{-\infty}^{t}\dfrac{\Gamma\Big(\dfrac{n+1}{2}\Big)}{\sqrt{n\pi}\Gamma\Big(\dfrac{n}{2}\Big)}\dfrac{1}{\Big(1+\dfrac{y^{2}}{n}\Big)^{\frac{n+1}{2}}}dy\\
\underset{\mathclap{\substack{\xuparrow[30pt]\\ \text{Teorema da}\\ \text{Convergência Dominada}}}}{}&=\int_{-\infty}^{t}\lim_{n\rightarrow+\infty}\dfrac{\Gamma\Big(\dfrac{n+1}{2}\Big)}{\sqrt{n\pi}\Gamma\Big(\dfrac{n}{2}\Big)}\dfrac{1}{\Big(1+\dfrac{y^{2}}{n}\Big)^{\frac{n+1}{2}}}dy\\
&=\star\star
\end{align*}

\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Considere a seguinte aproximação de Stirling (Conhecida como fórmula de Stirling): $$\Gamma(t+1)\approx \sqrt{2\pi t}\Big(\dfrac{t}{e}\Big)^{t}$$
Ou seja, $${\displaystyle \lim_{t\rightarrow+\infty} \dfrac{\Gamma(t+1)}{\sqrt{2t\pi}\Big(\dfrac{t}{e}\Big)^{t}}}=1$$
\end{block}
\pause
\begin{block}{}
Logo,
{\scriptsize
\begin{align*}
\lim_{n\rightarrow+\infty}\dfrac{\Gamma\Big(\dfrac{n+1}{2}\Big)}{\sqrt{n\pi}\Gamma\Big(\dfrac{n}{2}\Big)}\dfrac{1}{\Big(1+\dfrac{y^{2}}{n}\Big)^{\frac{n+1}{2}}}&=
\lim_{n\rightarrow+\infty}
\dfrac{\sqrt{2\pi}\Big(\frac{n-1}{2}\Big)^{\frac{n-1}{2}+\frac{1}{2}}e^{-(\frac{n-1}{2})}}{\sqrt{n}\sqrt{2\pi}\Big(\dfrac{n-2}{2}\Big)^{\frac{n-2}{2}+\frac{1}{2}}e^{-(\frac{n-2}{2})}}\dfrac{1}{\Big(1+\dfrac{y^{2}}{n}\Big)^{\frac{n+1}{2}}}\\
&=\star ~(\text{t da fórmula de Stirling será}~\frac{n-1}{2})
\end{align*}
}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
\begin{align*}
    \star&=\dfrac{e^{-\frac{1}{2}}}{\sqrt{2\pi}}{\displaystyle \lim_{n\rightarrow\infty}\dfrac{(n-1)^{\frac{n}{2}}}{(n-2)^{\frac{n}{2}}(n-2)^{-\frac{1}{2}}}}\dfrac{1}{\sqrt{n}}e^{-\frac{y^{2}}{2}}\\
    &=\dfrac{e^{-\frac{1}{2}}}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}{\displaystyle \lim_{n\rightarrow\infty}\Big(1+\dfrac{1}{n-2}\Big)^{\frac{n}{2}}}\\
    &=\dfrac{e^{-\frac{1}{2}}}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}e^{\frac{1}{2}}=\dfrac{e^{-\frac{y^{2}}{2}}}{\sqrt{2\pi}}
\end{align*}
\end{block}
\pause
\begin{block}{}
\justifying
Portanto, substituindo em $\star\star,$ temos 
\begin{align*}
\lim_{n\rightarrow+\infty}F_{T_{n}}(t)&=\int_{-\infty}^{t}\dfrac{e^{-\frac{y^{2}}{2}}}{\sqrt{2\pi}}dy
\end{align*}
Logo, $T_{n} \overset{D}{\rightarrow} N(0,1).$
\end{block}
\end{frame}

\begin{frame}{}
\begin{Teorema}
\justifying
Se $X_{n} \overset{P}{\rightarrow} X,~\text{então}~X_{n} \overset{D}{\rightarrow} X.$
\end{Teorema}
\pause
\begin{block}{Demonstração do Teorema 1}
	Seja \( x \) um ponto de continuidade de \( F_X(x) \), a função de distribuição acumulada (FDA) de \( X \). Queremos mostrar que \( F_{X_n}(x) \to F_X(x) \) à medida que \( n \to \infty \), onde \( F_{X_n}(x) \) é a FDA de \( X_n \).
	
	Para isso, partimos da definição de \( F_{X_n}(x) = P(X_n \leq x) \). Usaremos uma técnica dividindo a probabilidade em dois pedaços.
\end{block}
\end{frame}

\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
	Dividimos o evento \( \{X_n \leq x\} \) em dois subconjuntos: um onde \( |X_n - X| < \varepsilon \) e outro onde \( |X_n - X| \geq \varepsilon \). Assim, podemos reescrever:
	
	\begin{align*}
		F_{X_n}(x) &= P(X_n \leq x)\\ 
		&= P(\{X_n \leq x\} \cap \{|X_n - X| < \varepsilon\})\\ 
		&+ P(\{X_n \leq x\} \cap \{|X_n - X| \geq \varepsilon\})\\
		&\leq P(X \leq x + \varepsilon) + P(|X_n - X| \geq \varepsilon)
	\end{align*}
	
	Essa é uma decomposição da probabilidade em duas partes: uma onde \( X_n \) está "perto" de \( X \) (a diferença é menor que \( \varepsilon \)) e outra onde \( X_n \) está "longe" de \( X \) (a diferença é maior ou igual a \( \varepsilon \)).	
	\end{block}
\end{frame}


\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
	A probabilidade \( P(\{X_n \leq x\} \cap \{|X_n - X| < \varepsilon\}) \) pode ser estimada por \( P(X \leq x + \varepsilon) \).
	
	\[
	P(\{X_n \leq x\} \cap \{|X_n - X| < \varepsilon\}) \leq P(X \leq x + \varepsilon)
	\]
	
	Isso porque, quando \( |X_n - X| < \varepsilon \), sabemos que \( X_n \) está perto de \( X \), então \( X_n \leq x \) implica que \( X \leq x + \varepsilon \).	
	\end{block}
\end{frame}


\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
	O segundo termo, \( P(\{X_n \leq x\} \cap \{|X_n - X| \geq \varepsilon\}) \), é menor ou igual a \( P(|X_n - X| \geq \varepsilon) \), que é simplesmente a probabilidade de $X_{n}$ estar longe de $X.$ Essa probabilidade tende a 0 quando \( X_n \to X \) em probabilidade, mas por enquanto, deixamos essa expressão como está:
	
	\[
	P(\{X_n \leq x\} \cap \{|X_n - X| \geq \varepsilon\}) \leq P(|X_n - X| \geq \varepsilon)
	\]	
	\end{block}
\end{frame}

\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
	Juntando as duas estimativas, temos:
	
	\[
	F_{X_n}(x) \leq P(X \leq x + \varepsilon) + P(|X_n - X| \geq \varepsilon)
	\]
	
	Esta é a estimativa superior para \( F_{X_n}(x) \).
	
	
	\begin{itemize}
		\item O primeiro termo, \( P(X \leq x + \varepsilon) \), representa o evento de que \( X \) está um pouco acima de \( x \). Isso é um "ajuste", pois estamos lidando com \( X_n \) próximo de \( X \).\pause
		\item O segundo termo, \( P(|X_n - X| \geq \varepsilon) \), é a probabilidade de que \( X_n \) esteja muito distante de \( X \), ou seja, mais de \( \varepsilon \) de diferença.	
	\end{itemize}
	\end{block}
\end{frame}

\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
		Quando \( X_n \to X \) em probabilidade, sabemos que \( P(|X_n - X| \geq \varepsilon) \to 0 \) conforme \( n \to \infty \). Portanto, com base nessa desigualdade, podemos concluir:
		
		\[
		\lim_{n \to \infty} F_{X_n}(x) \leq F_X(x + \varepsilon)
		\]
		
		Isso nos dá a estimativa superior (upper bound) da função de distribuição acumulada de \( X_n \).
	\end{block}
\end{frame}


\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
	Agora, para obter a **estimativa inferior**, começamos reescrevendo \( P(X_n \leq x) \) utilizando o complemento:
	
	\[
	P(X_n \leq x) = 1 - P(X_n > x)
	\]
	
	Dividimos a probabilidade \( P(X_n > x) \) em dois pedaços:
	
	\[
	P(X_n > x) = P(\{X_n > x\} \cap \{|X_n - X| < \varepsilon\}) + P(\{X_n > x\} \cap \{|X_n - X| \geq \varepsilon\})
	\]	
	\end{block}
\end{frame}


\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
	\begin{itemize}
		\item A primeira parte \( P(\{X_n > x\} \cap \{|X_n - X| < \varepsilon\}) \) considera os casos em que \( X_n \) está próximo de \( X \) (a diferença é menor que \( \varepsilon \)) e, ao mesmo tempo, \( X_n > x \).
		\pause
		\item A segunda parte \( P(\{X_n > x\} \cap \{|X_n - X| \geq \varepsilon\}) \) considera os casos em que \( X_n \) e \( X \) estão distantes mais de \( \varepsilon \).
	\end{itemize}	
	\end{block}
\end{frame}

\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
	Como \( P(\{X_n > x\} \cap \{|X_n - X| < \varepsilon\}) \) é menor que \( P(X > x - \varepsilon) \), podemos usar a seguinte desigualdade:
	
	\[
	P(X_n > x) \leq P(X \geq x - \varepsilon) + P(|X_n - X| \geq \varepsilon)
	\]
	
	- O primeiro termo, \( P(X \geq x - \varepsilon) \), é a probabilidade de \( X \) ser maior ou igual a \( x - \varepsilon \). Isso é uma aproximação para lidar com o fato de que \( X_n \) está próximo de \( X \).
	- O segundo termo, \( P(|X_n - X| \geq \varepsilon) \), representa a probabilidade de \( X_n \) estar distante de \( X \) (mais de \( \varepsilon \)).	
	\end{block}
\end{frame}

\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
	Assim, podemos expressar \( P(X_n \leq x) \) como:
	
	\[
	P(X_n \leq x) = 1 - P(X_n > x)
	\]
	
	Substituímos o limite que encontramos para \( P(X_n > x) \):
	
	\[
	P(X_n \leq x) \geq 1 - P(X \geq x - \varepsilon) - P(|X_n - X| \geq \varepsilon)
	\]
	
	Ou, de forma mais compacta:
	
	\[
	F_{X_n}(x) \geq F_X(x - \varepsilon) - P(|X_n - X| \geq \varepsilon)
	\]	
	\end{block}
\end{frame}

\begin{frame}{}
	\begin{block}{Demonstração do Teorema 1}
	Sabemos que, como \( X_n \to X \) em probabilidade, temos \( P(|X_n - X| \geq \varepsilon) \to 0 \) conforme \( n \to \infty \). Assim, no limite:
	
	\[
	\lim_{n \to \infty} F_{X_n}(x) \geq F_X(x - \varepsilon)
	\]
	
	
	Agora, combinamos as duas estimativas (superior e inferior) que obtivemos:
	
	\[
	F_X(x - \varepsilon) \leq \lim_{n \to \infty} F_{X_n}(x) \leq F_X(x + \varepsilon)
	\]
	
	Finalmente, fazendo \( \varepsilon \to 0 \), chegamos à conclusão desejada:
	
	\[
	\lim_{n \to \infty} F_{X_n}(x) = F_X(x)
	\]	
	\end{block}
\end{frame}


\begin{frame}{}
\begin{Teorema}
\justifying
Se $X_{n} \overset{D}{\rightarrow} a,~\text{então}~X_{n} \overset{P}{\rightarrow} a,~a$ constante.
\end{Teorema}
\end{frame}

\begin{frame}{}
\begin{Teorema}
\justifying
Se $X_{n} \overset{D}{\rightarrow} X~\text{e}~Y_{n} \overset{P}{\rightarrow} 0~\text{então}~X_{n}+Y_{n} \overset{D}{\rightarrow} X.$
\end{Teorema}
\end{frame}

\begin{frame}{}
\begin{Teorema}
\justifying
Se $X_{n} \overset{D}{\rightarrow} X~\text{e}~g~\text{é uma função contínua no suporte de}~X,~\text{então}~$ $$g(X_{n}) \overset{D}{\rightarrow} g(X).$$
\end{Teorema}
\end{frame}

\begin{frame}{Teorema de Slutsky}
\begin{Teorema}
\justifying
Sejam $X_{n},~A_{n}$ e $B_{n},$ variáveis aleatórias com $X_{n} \overset{D}{\rightarrow} X,~A_{n} \overset{P}{\rightarrow} a~\text{e}~B_{n} \overset{P}{\rightarrow} b,~a,b$~constantes reais. Então, $$A_{n}X_{n}+B_{n} \overset{D}{\rightarrow} aX+b.$$
\end{Teorema}
\end{frame}

\begin{frame}{}
\begin{block}{\Home}
	\nocite{hogg}
\justifying
Exercícios 5.2.2, 5.2.3, 5.2.6, 5.2.12, 5.2.15, 5.2.17, 5.2.19 e 5.2.20
\end{block}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{\bf Referências}
\printbibliography
\end{frame}


\end{document}
