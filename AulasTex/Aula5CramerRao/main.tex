\documentclass[12pt]{beamer}

\input{../Configuracoes/layout}


\title{Inferência Estatística II}
\author{Prof. Fernando de Souza Bastos\texorpdfstring{\\ fernando.bastos@ufv.br}{}}
\institute{Departamento de Estatística\texorpdfstring{\\ Programa de Pós-Graduação em Estatística Aplicada e Biometria}\texorpdfstring{\\ Universidade Federal de Viçosa}{}\texorpdfstring{\\ Campus UFV - Viçosa}{}}
\date{}
\newcommand\mytext{Aula 4}
\newcommand\mytextt{Fernando de Souza Bastos}
\newcommand\mytexttt{\url{https://est711.github.io/}}


\begin{document}
%\SweaveOpts{concordance=TRUE}

\frame{\titlepage}

\begin{frame}{}
\frametitle{\bf Sumário}
\tableofcontents
\end{frame}

\section{Limite Inferior de Cramér - Rao e Eficiência}
\subsection{Informação de Fisher}
\begin{frame}{Limite Inferior de Cramér - Rao}
\begin{block}{}
\justifying
O limite inferior de Cramér - Rao é o menor valor que a variância de um estimador pode assumir. Sob certas condições de regularidade mostraremos que os estimadores de Máxima Verossimilhança (EMV) atingem assintoticamente este limite inferior.
\end{block}
\end{frame}

\begin{frame}{Mais Condições de Regularidade}
\begin{block}{}
\justifying
Considere $X$ uma v.a.c. com densidade $f(x,\theta),~\theta\in \Omega,~\Omega$ é um intervalo aberto. Adicionalmente, considere também:
\begin{description}
\item[(R3)~] A densidade $f(x,\theta)$ é duas vezes diferenciável com relação a $\theta.$\pause
\item[(R4)~]A integral $\int f(x,\theta)dx$ pode ser derivada duas vezes sob o sinal de integração como função de $\theta.$
\end{description}
\textbf{Obs:} O caso discreto segue de forma similar.
\end{block}
\end{frame}

\begin{frame}{Informação de Fisher}
\begin{block}{}
\justifying
Considere a identidade:
\begin{align*}
    \int_{-\infty}^{\infty}f(x,\theta)dx=1&\Rightarrow \dfrac{\partial}{\partial\theta}\int_{-\infty}^{\infty}f(x,\theta)dx=0\\
    \pause
    &\Implica{\text{Sob}~(R4)}\int_{-\infty}^{\infty}\dfrac{\partial}{\partial\theta}f(x,\theta)dx=0\\
    \pause
    &\SeSe \int_{-\infty}^{\infty}\Big\{\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}\Big\}f(x,\theta)dx=0\\
    \pause
    &\SeSe E\Big(\Big\{\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}\Big\}\Big)=0\qquad (\star)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Tomando novamente a derivada com relação a $\theta,$ obtemos:
{\footnotesize
\begin{align*}
    \int_{-\infty}^{\infty}\Big\{\dfrac{\partial^{2}}{\partial\theta^{2}}\log{f(x,\theta)}\Big\}f(x,\theta)dx+\int_{-\infty}^{\infty}\Big\{\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}\Big\}\dfrac{\partial}{\partial\theta}f(x,\theta)dx=0~\quad (\star\star)
\end{align*}}
\pause
{\footnotesize
\begin{align*}
\int_{-\infty}^{\infty}\Big\{\dfrac{\partial^{2}}{\partial\theta^{2}}\log{f(x,\theta)}\Big\}f(x,\theta)dx+\int_{-\infty}^{\infty}\Big\{\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}\Big\}\Big\{\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}\Big\}f(x,\theta)dx=0\\
\end{align*}}
\pause 
{\small
\begin{align*}
\int_{-\infty}^{\infty}\Big\{\dfrac{\partial^{2}}{\partial\theta^{2}}\log{f(x,\theta)}\Big\}f(x,\theta)dx+\int_{-\infty}^{\infty}\Big\{\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}\Big\}^{2}f(x,\theta)dx=0\\
\end{align*}}
\end{block}
\end{frame}

\begin{frame}{Informação de Fisher}
\begin{block}{}
\justifying
O segundo termo, denotado por $I(\theta),$ dado por $$I(\theta)=\int_{-\infty}^{\infty}\Big\{\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}\Big\}^{2}f(x,\theta)dx=E\Big[\Big(\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}\Big)^{2}\Big]$$ é conhecido como Informação de Fisher. Usando ($\star$), temos que
$$I(\theta)=Var\Big(\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}\Big).$$
\end{block}
\end{frame}

\subsection{Função Escore}
\begin{frame}{Função Escore}
\begin{block}{}
\justifying
Usando ($\star\star$), temos que 
$$I(\theta)=-\int_{-\infty}^{\infty}\Big\{\dfrac{\partial^{2}}{\partial\theta^{2}}\log{f(x,\theta)}\Big\}f(x,\theta)dx$$
\end{block}
\pause
\begin{block}{}
\justifying
\begin{itemize}
    \item $\dfrac{\partial}{\partial\theta}\log{f(x,\theta)}$ é conhecido como função Escore.\pause
    \item Um estimador de máxima verossimilhança é solução da equação 
    \begin{align*}
        \Sumi \dfrac{\partial}{\partial\theta}\log{f(x,\theta)}=0
    \end{align*}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Exemplo}
\begin{block}{Calcule $I(\theta)$}
\justifying
Suponha $X\sim Bernoulli(\theta)$ com $P(X=0)=1-P(X=1)=1-\theta.$
\end{block}
\pause
\begin{block}{}
\justifying
\begin{align*}
\log{f(x,\theta)}&=x\log{\theta}+(1-x)\log{(1-\theta)}\\
\Rightarrow \dfrac{\partial^{2}}{\partial\theta^{2}}\log{f(x,\theta)}&=-\dfrac{x}{\theta^{2}}-\dfrac{1-x}{(1-\theta)^{2}}\\
\Rightarrow I(\theta)&=-E\Big(\dfrac{\partial^{2}}{\partial\theta^{2}}\log{f(x,\theta)}\Big)=\dfrac{1}{\theta(1-\theta)}
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Sejam $X_{1},\ldots,X_{n}$ v.a's iid com densidade $f(x,\theta)$ e $\vecX=(X_{1},\ldots,X_{n})^{T}.$ Nesse caso,
\begin{align*}
    \dfrac{\partial\ell(\theta,\vecX)}{\partial\theta}=\Sumi\dfrac{\partial}{\partial\theta}\log{f(X_{i},\theta)}
\end{align*}
Como cada termo na soma tem a mesma variância $I(\theta),$ segue que 
\begin{align*}
    Var\Big(\dfrac{\partial\ell(\theta,\vecX)}{\partial\theta}\Big)&\stackrel{\text{iid}}{=}\Sumi \underbrace{Var\Big(\dfrac{\partial}{\partial\theta}\log{f(X_{i},\theta)}\Big)}_{I(\theta)}\\
    &=\Sumi I(\theta)=nI(\theta)
\end{align*}
\end{block}
\pause
\begin{block}{}
    Assim, a informação em uma amostra aleatória de tamanho $n$ é $n$ vezes a informação em uma amostra de tamanho $1.$
\end{block}
\end{frame}

\subsection{Limite Inferior de Cramér - Rao}
\begin{frame}{Limite Inferior de Cramér - Rao}
\begin{block}{}
\justifying
Sejam $X_{1},\ldots,X_{n}$ v.a's iid com densidade $f(x,\theta),~\theta\in \Omega.$ Assuma que as condições de regularidade (R0) à (R4) valem. Seja $Y=u(X_{1},\ldots,X_{n})$ uma estatística com média $E(Y)=k(\theta).$ Então, 
\begin{align*}
    Var(Y)\geq \dfrac{[k^{'}(\theta)]^{2}}{nI(\theta)}
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Demonstração}
\vspace{-0.2cm}
\begin{block}{}
\justifying
\begin{align*}
k(\theta) = \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} u(x_1, \ldots, x_n) f(x_1; \theta) \cdots f(x_n; \theta)dx_1 \cdots dx_n
\end{align*}
Diferenciando com respeito a $\theta,$ temos
{\small
\begin{align*}
k^{'}(\theta) &= \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} \frac{\partial u(x_1, \ldots, x_n)}{\partial\theta} f(x_1; \theta) \cdots f(x_n; \theta)dx_1 \cdots dx_n\\
&+\int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} u(x_1, \ldots, x_n)\frac{\partial }{\partial\theta}\left[f(x_1; \theta) \cdots f(x_n; \theta)\right] dx_1 \cdots dx_n
\end{align*}
}
Notem que, $${\small \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} \frac{\partial u(x_1, \ldots, x_n)}{\partial\theta} f(x_1; \theta) \cdots f(x_n; \theta)dx_1 \cdots dx_n=0,}$$ pois $u(x_1, \ldots, x_n)$ não depende de $\theta.$
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Logo, 
{\small 
\begin{align*}
k^{'}(\theta) &= \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} u(x_1, \ldots, x_n)\frac{\partial }{\partial\theta}\left[f(x_1; \theta) \cdots f(x_n; \theta)\right] dx_1 \cdots dx_n\\
\SeSe k^{'}(\theta) &= \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} u(x_1, \ldots, x_n)\frac{\partial }{\partial\theta}\Prodi f(x_i; \theta) dx_1 \cdots dx_n\\
\Rightarrow k^{'}(\theta) &= \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} u(x_1, \ldots, x_n)\Sumi \frac{\partial \log{f(x_{i},\theta)}}{\partial\theta}\Prodi f(x_i; \theta) dx_1 \cdots dx_n
\end{align*}
}
Seja $Z=\Sumi \frac{\partial \log{f(x_{i},\theta)}}{\partial\theta}.$ Logo,
\begin{align*}
k^{'}(\theta) &= E(YZ)\qquad~(\star\star\star)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\vspace{-0.2cm}
\begin{block}{}
\justifying
Como visto anteriormente, $E(Z)=0$ e $Var(Z)=nI(\theta),$ além disso, temos que a correlação entre $Y$ e $Z$ é dada por,
\begin{align*}
    \rho&=Cor(Y,Z)=\dfrac{COV(Y,Z)}{\sqrt{Var(Y)Var(Z)}}\\
    COV(Y,Z)&=E(YZ)-E(Y)E(Z)
\end{align*}
Como $E(Z)=0,$ temos
\begin{align*}
    \rho&=\dfrac{E(YZ)}{\sigma_{Y}\sqrt{nI(\theta)}}\\
    \Rightarrow E(YZ)&=\rho \sigma_{Y}\sqrt{nI(\theta)}\qquad (\star\star\star\star)
\end{align*}
\end{block}
\pause
\begin{block}{}
\justifying
Substituindo, $(\star\star\star\star)$ em $(\star\star\star),$ temos que 
\begin{align*}
k^{'}(\theta)&=\rho \sigma_{Y}\sqrt{nI(\theta)}\Rightarrow \dfrac{k^{'}(\theta)}{\sigma_{Y}\sqrt{nI(\theta)}} =\rho  
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Usando o fato de que $\rho^{2}\leq 1,$ temos
\begin{align*}
\dfrac{[k^{'}(\theta)]^{2}}{\sigma_{Y}^{2}nI(\theta)} =\rho^{2}\leq 1\Rightarrow  \sigma_{Y}^{2}\geq  \dfrac{[k^{'}(\theta)]^{2}}{nI(\theta)}
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Corolário}
\begin{block}{Corolário:}
\justifying
Sob as condições do teorema anterior, se $Y$ é um estimador não viesado de $\theta,$ ou seja, $k(\theta)=\theta,$ então $Var(Y)\geq \dfrac{1}{nI(\theta)}.$
\end{block}
\pause
\begin{definicao}
\justifying
    Seja $Y$ um estimador não viesado de $\theta.$ A estatística $Y$ é um estimador eficiente de $\theta$ se, e somente se, sua variância é igual ao limite inferior de Cramér-Rao.
\end{definicao}
\pause
\begin{definicao}
\justifying
    A razão entre o limite inferior de Cramér-Rao e a variância de um estimador, digamos $Y,$ é chamada de eficiência do estimador $Y.$
\end{definicao}
\end{frame}

\begin{frame}{Exemplo 1}
\begin{block}{}
\justifying
$X_{1},\ldots,X_{n}\sim$ Poisson($\theta$).
\begin{align*}
    P(X_{1}=k)=\dfrac{e^{-\theta}\theta^{k}}{k!},~\theta>0, k=0,1,\ldots
\end{align*}
O estimador de Máxima Verossimilhança de $\theta$ é $\hat{\theta}=\Bar{X},~E(\Bar{X})=\theta,~Var(\Bar{X})=\dfrac{\theta}{n}.$ Logo, $I(\theta)=\dfrac{1}{\theta}$ e, portanto, o limite inferior de Cramér-Rao é $\dfrac{1}{n\frac{1}{\theta}}=\dfrac{\theta}{n}.$ Como, $var(\Bar{X})$ é igual ao limite inferior de Cramér-Rao, então $\Bar{X}$ é um estimador eficiente para $\theta.$ 
\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{Contagem de Acidentes de Trânsito}
\justifying
Suponha que você esteja analisando a ocorrência de acidentes de trânsito em uma determinada interseção ao longo de um período de tempo. Você deseja modelar a distribuição das ocorrências de acidentes usando um modelo Poisson. O parâmetro que você está interessado em estimar é a taxa média de acidentes $\lambda$ nessa interseção.
\end{block}
\pause
\begin{block}{}
\justifying
Durante um mês, você registra o número de acidentes que ocorreram diariamente na interseção. Os dados coletados são: $2, 1, 3, 0, 2, 4, 1, 2, 3, 1, 2, 0, 1, 2, 2, 3, 0, 1, 1, 2.$
\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{}
\justifying
O modelo Poisson é uma distribuição que descreve o número de eventos raros que ocorrem em um intervalo de tempo ou espaço fixo, quando esses eventos ocorrem de forma independente e com uma taxa média constante. A probabilidade de observar exatamente $k$ eventos em um intervalo é dada pela fórmula da distribuição Poisson:
\[ P(X = k) = \frac{e^{-\lambda} \lambda^k}{k!} \]
\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{}
\justifying
O parâmetro $\lambda$ representa a taxa média de acidentes por dia. Você deseja estimar esse parâmetro com base nos dados coletados.
\end{block}
\pause
\begin{block}{}
\justifying
A função de verossimilhança é a expressão que relaciona a probabilidade dos dados observados com o parâmetro $\lambda$. Para um conjunto de dados, a função de verossimilhança é o produto das probabilidades de observar cada valor específico, dada a distribuição Poisson. A função de verossimilhança para os dados coletados é:
\[L(\lambda) = \frac{e^{-20\lambda} \lambda^{\sum{k}}}{k_1! \cdot k_2! \cdot \ldots \cdot k_n!}\]
onde $\sum{k}$ representa a soma dos valores dos acidentes observados e $k_1, k_2, \ldots, k_n$ são esses valores individuais.
\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{Estimador de $\lambda$}
\justifying
O estimador de máxima verossimilhança para $\lambda$ em um modelo Poisson é a média dos valores observados. Neste caso, a soma dos acidentes observados é $33$ e o número de dias é 20, então o estimador de $\lambda$ é:
\[\hat{\lambda} = \frac{\sum{k}}{n} = \frac{33}{20} = 1.65\]
\end{block}
\pause
\begin{block}{Informação de Fisher}
\justifying
A informação de Fisher para um modelo Poisson é dada pelo inverso da variância da distribuição Poisson, que é $\lambda$. Portanto, a informação de Fisher é igual a $1/\lambda$.
\[ \text{Informação de Fisher (} I(\lambda) \text{)} = \frac{1}{\hat{\lambda}} = \frac{1}{1.65} \approx 0.61 \]
\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{}
\justifying
No exemplo específico, a informação de Fisher, indica a precisão da estimativa com base na sensibilidade da distribuição Poisson à variação da taxa média.
\end{block}
\pause
\begin{block}{}
\justifying
O ideal é comparar a Informação de Fisher de duas ou mais amostras distintas ou quando se deseja avaliar a precisão das estimativas em diferentes cenários. Ao comparar duas amostras distintas, a informação de Fisher pode ajudar a determinar qual amostra fornece informações mais úteis sobre os parâmetros em questão. Maior informação de Fisher indica uma amostra mais informativa, que proporciona estimativas mais precisas dos parâmetros.
\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{}
\justifying
Em resumo, se a informação de Fisher for alta para um determinado parâmetro, isso indica que os dados têm uma forte capacidade de distinguir diferentes valores desse parâmetro. Por outro lado, uma informação de Fisher baixa sugere que os dados têm menos poder para discernir variações no parâmetro, resultando em estimativas menos precisas.
\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{Cálculo e interpretação da Função Escore}
\justifying
A função escore é a derivada da função de log-verossimilhança em relação ao parâmetro de interesse. No caso do modelo Poisson, a função de log-verossimilhança para os dados coletados é:

\[
\ln L(\lambda) = -20\lambda + \sum{k} \ln(\lambda) - \ln(k_1!) - \ln(k_2!) - \ldots - \ln(k_n!)
\]

A derivada da função de log-verossimilhança em relação a $\lambda$ é a função escore:

\[
\text{Escore}(\lambda) = -20 + \frac{\sum{k}}{\lambda}
\]
\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{Cálculo e interpretação da Função Escore}
\justifying
No contexto deste exemplo, a função escore expressa como a função de log-verossimilhança muda à medida que ajustamos o parâmetro $\lambda$, que representa a taxa média de acidentes. O valor da função escore nos diz em que direção e magnitude a probabilidade de observar os dados muda conforme alteramos $\lambda$.
\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{Cálculo e interpretação da Função Escore}
\justifying
No cálculo acima, a função escore é $-20 + \frac{\sum{k}}{\lambda}$. Isso significa que quando você aumenta $\lambda$, o valor do escore diminui, indicando que a probabilidade de observar os dados diminui à medida que a taxa média de acidentes aumenta. Da mesma forma, quando você diminui $\lambda$, o valor do escore aumenta, indicando que a probabilidade de observar os dados aumenta à medida que a taxa média de acidentes diminui.
\end{block}
\end{frame}

\begin{frame}{Mais uma condição de Regularidade}
\begin{block}{}
{\noindent
\begin{description}
\justifying
\item[(R5)~]A densidade $f(x,\theta)$ é três vezes diferenciável com relação a $\theta.$ Além disso, para todo $\theta\in \Omega$ suponha a existência de uma constante $c$ e uma função $M(x)$ tais que:
\begin{align*}
    \Bigg|\dfrac{\partial^{3}\log{f(x,\theta)}}{\partial\theta^{3}}\Bigg|\leq M(x),~\text{em que}~
    E_{\theta_{0}}(M(X))<\infty\\
    \text{para todo}~\theta\in(\theta_{0}-c,\theta_{0}+c)~\text{com}~x~\text{no suporte de}~X.
\end{align*}
\end{description}}
\end{block}
\end{frame}

\begin{frame}{Teorema}
\vspace{-0.2cm}
\begin{Teorema}
\justifying
Assuma que $X_{1},\ldots,X_{n}$ é uma amostra aleatória de uma variável aleatória $X$ com densidade $f(x,\theta_{0}),$ para $\theta_{0}\in\Omega$ e que as condições de regularidade (R0) à (R5) valem. Além disso, assuma que $0<I(\theta)<+\infty.$ Então, qualquer sequência de soluções consistentes $\hat{\theta}_{n}$ da equação de estimação de Máxima Verossimilhança satisfaz:
\begin{align*}
    \sqrt{n}(\hat{\theta}_{n}-\theta_{0})\ConvD N(0,\dfrac{1}{I(\theta_{0})})
\end{align*}
\begin{align*}
\SetaUP{\text{para}~\hat{\theta}_{n}\sim N(0,\frac{1}{I(\theta_{0})})}{\textbf{Obs:}}Var(\sqrt{n}\hat{\theta}_{n})\approx\dfrac{1}{I(\theta_{0})}\Rightarrow Var(\hat{\theta}_{n})\approx\dfrac{1}{nI(\hat{\theta}_{n})},~E(\hat{\theta}_{n})=0.    
\end{align*}
\end{Teorema}
\end{frame}

\begin{frame}{Demonstração}
\begin{block}{}
\justifying
Expandindo $\ell^{'}(\hat{\theta}_{n})$ em torno de $\theta_{0}$ obtemos que
\begin{align*}
    \ell^{'}(\hat{\theta}_{n})=\ell^{'}(\theta_{0})+(\hat{\theta}_{n}-\theta_{0})\ell^{''}(\theta_{0})+
    \dfrac{(\hat{\theta}_{n}-\theta_{0})^{2}}{2}\ell^{'''}(\theta_{n}^{*}),
\end{align*}
em que $\theta_{0}^{*}$ pertence ao intervalo entre $\theta_{0}$ e $\hat{\theta}_{n}.$ Como $\ell^{'}(\hat{\theta}_{n})=0,$ temos
\begin{align*}
   0&= \ell^{'}(\theta_{0})+(\hat{\theta}_{n}-\theta_{0})\ell^{''}(\theta_{0})+
    \dfrac{(\hat{\theta}_{n}-\theta_{0})^{2}}{2}\ell^{'''}(\theta_{n}^{*})\\
    \Rightarrow -\ell^{'}(\theta_{0})&=\sqrt{n}(\hat{\theta}_{n}-\theta_{0})\Big[\dfrac{1}{\sqrt{n}}\ell^{''}(\theta_{0})+
    \dfrac{(\hat{\theta}_{n}-\theta_{0})}{2\sqrt{n}}\ell^{'''}(\theta_{n}^{*})\Big]
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
\begin{align*}
   \Rightarrow \sqrt{n}(\hat{\theta}_{n}-\theta_{0})&=\dfrac{-\ell^{'}(\theta_{0})}{\dfrac{1}{\sqrt{n}}\ell^{''}(\theta_{0})+
    \dfrac{(\hat{\theta}_{n}-\theta_{0})}{2\sqrt{n}}\ell^{'''}(\theta_{n}^{*})}\\
    \Rightarrow \sqrt{n}(\hat{\theta}_{n}-\theta_{0})&=\dfrac{\dfrac{-\ell^{'}(\theta_{0})}{\sqrt{n}}}{\dfrac{1}{n}\ell^{''}(\theta_{0})+
    \dfrac{(\hat{\theta}_{n}-\theta_{0})}{2n}\ell^{'''}(\theta_{n}^{*})}\\
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{Notem que:}
\justifying
\begin{enumerate}
    \item
\begin{align*}
    \dfrac{-\ell^{'}(\theta_{0})}{\sqrt{n}}=\dfrac{1}{\sqrt{n}}\Sumi\dfrac{\partial}{\partial\theta_{0}}\log{f(x,\theta_{0})}    \xrightarrow[n\rightarrow\infty]{TCL}N(0,I(\theta_{0}))
\end{align*}
\item
\begin{align*}
    \dfrac{-\ell^{''}(\theta_{0})}{n}&=-\dfrac{1}{n}\Sumi\dfrac{\partial^{2}}{\partial\theta_{0}^{2}}\log{f(x,\theta_{0})}    \SetaUP{LFGN}{\xrightarrow[n\rightarrow\infty]{P}}-E\Bigg(\dfrac{\partial^{2}}{\partial\theta_{0}^{2}}\log{f(x_{i},\theta_{0})}\Bigg)\\
    &=I(\theta_{0})
\end{align*}
\seti
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{Notem que:}
\justifying
\begin{enumerate}
\conti
    \item
\begin{align*}
    (\hat{\theta}_{n}-\theta_{0})\ConvP 0~\text{e}~
    \dfrac{1}{n}\ell^{'''}(\hat{\theta}_{n}^{*})~\text{é limitado em probabilidade. Daí}
\end{align*}
\pause
\begin{align*}
    (\hat{\theta}_{n}-\theta_{0})\dfrac{1}{n}\ell^{'''}(\hat{\theta}_{n}^{*})\ConvP 0
\end{align*}
\end{enumerate}
\end{block}
\pause
\begin{block}{}
\justifying
Logo, por Slutsky,
\begin{align*}
  \sqrt{n}(\hat{\theta}_{n}-\theta_{0})\ConvD N(0,\dfrac{1}{I(\theta_{0})}) \qquad\cqd 
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{definicao}
\justifying
$X_{i}\Sim f(x,\theta).$Suponha que $\hat{\theta}_{1n}=\hat{\theta}_{1n}(X_{1},\ldots,X_{n})$ é um estimador de $\theta_{0}$ tal que 
\begin{align*}
  \sqrt{n}(\hat{\theta}_{1n}-\theta_{0})\ConvD N(0,\sigma^{2}_{\hat{\theta}_{1n}}).
\end{align*}
Então,
\begin{enumerate}
    \item A eficiência relativa (ou eficiência assintótica) de $\hat{\theta}_{1n}$ é definida como $e(\hat{\theta}_{1n})=\dfrac{\frac{1}{I(\theta_{0})}}{\sigma_{\hat{\theta}_{1n}}^{2}}$
    \item O estimador $\hat{\theta}_{1n}$ é dito ser assintoticamente eficiente se $e(\hat{\theta}_{1n})=1.$
    \seti
\end{enumerate}
\end{definicao}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
\begin{enumerate}
\conti
\item Se $\hat{\theta}_{2n}$ é um outro estimador satisfazendo
\begin{align*}
    \sqrt{n}(\hat{\theta}_{2n}-\theta_{0})\ConvD N(0,\sigma^{2}_{\hat{\theta}_{2n}}),
\end{align*}
então a eficiência relativa assintótica de $\hat{\theta}_{1n}$ e $\hat{\theta}_{2n}$ é definida como
\begin{align*}
e(\hat{\theta}_{1n},\hat{\theta}_{2n})=\dfrac{\sigma_{\hat{\theta}_{2n}}^{2}}{\sigma_{\hat{\theta}_{1n}}^{2}}.
\end{align*}
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Exercícios}
\begin{block}{\Home}
\justifying
Exercícios 6.2: 1, 3 (item 1), 7 (letras a, b e c), 8, 9, 10, 11, 12, 16
\nocite{hogg, casella2021statistical, bolfarine}
\end{block}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{\bf Referências}
\printbibliography
\end{frame}


\end{document}
