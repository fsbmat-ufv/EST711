\documentclass[12pt]{beamer}

\input{../Configuracoes/layout}
\setLayoutColor{1} 


\title{Inferência Estatística II}
\author{Prof. Fernando de Souza Bastos\texorpdfstring{\\ fernando.bastos@ufv.br}{}}
\institute{Departamento de Estatística\texorpdfstring{\\ Programa de Pós-Graduação em Estatística Aplicada e Biometria}\texorpdfstring{\\ Universidade Federal de Viçosa}{}\texorpdfstring{\\ Campus UFV - Viçosa}{}}
\date{}
\newcommand\mytext{Aula 7}
\newcommand\mytextt{Fernando de Souza Bastos}
\newcommand\mytexttt{\url{https://est711.github.io/}}


\begin{document}
%\SweaveOpts{concordance=TRUE}

\frame{\titlepage}

\begin{frame}{}
\frametitle{\bf Sumário}
\tableofcontents
\end{frame}

\section{Intervalo de Confiança}
\begin{frame}{}
\begin{block}{}
\justifying
Discutiremos inicialmente, dois casos. O primeiro baseado no Teorema Central do Limite. O segundo iremos assumir que a amostra aleatória provém de uma distribuição normal.
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Seja \seqX~ uma a.a. de uma distribuição dependendo de um parâmetro $\theta.$ Seja $\theta_{0}$ o valor verdadeiro do parâmetro desconhecido $\theta.$ Seja $T$ uma estatística para $\theta_{0}$ satisfazendo a seguinte convergência em distribuição
\begin{align*}
    \sqrt{n}(T-\theta_{0})\ConvD N(0,\sigma^{2}_{T})\qquad (\star)
\end{align*}
Em que $\sigma^{2}_{T}$ é a variância assintótica de $\sqrt{n}T.$ Para nossos propósitos iniciais suponha que $\sigma^{2}_{T}$ é conhecido (geralmente ele é desconhecido)
\end{block}
\pause 
\begin{block}{}
\justifying
De $(\star),$ temos que 
\begin{align*}
    \dfrac{\sqrt{n}(T-\theta_{0})}{\sigma_{T}}\ConvD N(0,1)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Seja $z_{\alpha}$ o quantil $\alpha$ da distribuição normal padrão. Ou seja, se $Z\sim N(0,1),$ então $P(Z<z_{\alpha})=\alpha.$

\begin{figure}[H]
\centering
\begin{tikzpicture}[line cap=round,line join=round,x=1.9cm,y=4*1.9cm,scale=0.8]
\draw[->,color=black,line width=1.0pt] (-3.5,-0.01) -- (3.7,-0.01);
\draw (3.8,-0.01) node[] {$\R$};
\draw[shift={(0,0)},color=black,line width=1.0pt] (0,0.02) -- (0,-0.02) node[below] { $0$};
\draw[smooth,samples=100,domain=-3.5:3.5,line width=1.0pt] plot(\x,{(1/sqrt(2*pi))*exp((-(\x)^2)/2)});
\draw[fill=black,fill opacity=0.3, smooth,samples=50,domain=-3.5:-1.5] plot(\x,{(1/sqrt(2*pi))*exp((-(\x)^2)/2)}) -- (-1.5,0) -- (-3.5,0) -- cycle;
\draw [dash pattern=on 3pt off 3pt,line width=1.0pt] (-1.5,0) -- (-1.5,0.3989);
%\draw (-2.5,0.2393) node[right] {$RRH_0$};
%\draw (-0.0,0.2393) node[] {$RNRH_0$};
%\draw (1.55,0.2393) node[] {$RNRH_0$};
\draw [->] (-1.9,0.0399) -- (-2.2,0.1197);
\draw (-2.15,0.1197) node[left] {$\alpha$};
\draw (-1.5,0) node[below] {$z_{\alpha}$};
\end{tikzpicture}
%\caption{Região crítica para o teste $Z$, para uma média, unilateral à esquerda.}\label{Fig:THZRCEsq}
\end{figure}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Como 
\begin{align*}
    \dfrac{\sqrt{n}(T-\theta_{0})}{\sigma_{T}}\ConvD N(0,1),
\end{align*}
Temos que 
\begin{align*}
P(-z_{1-\frac{\alpha}{2}}<\dfrac{\sqrt{n}(T-\theta_{0})}{\sigma_{T}}<z_{1-\frac{\alpha}{2}})\approx 1-\alpha
\end{align*}

\begin{figure}[H]
\centering
\begin{tikzpicture}[line cap=round,line join=round,x=1.9cm,y=4*1.9cm,scale=0.8]
\draw[->,color=black,line width=1.0pt] (-3.5,-0.01) -- (3.7,-0.01);
\draw (3.8,-0.01) node[] {$\R$};
\draw[shift={(0,0)},color=black,line width=1.0pt] (0,0.02) -- (0,-0.02) node[below] {$0$};
\draw[smooth,samples=100,domain=-3.5:3.5,line width=1.0pt] plot(\x,{(1/sqrt(2*pi))*exp((-(\x)^2)/2)});
\draw[fill=black,fill opacity=0.3, smooth,samples=50,domain=-3.5:-1.5] plot(\x,{(1/sqrt(2*pi))*exp((-(\x)^2)/2)}) -- (-1.5,0) -- (-3.5,0) -- cycle;
\draw [dash pattern=on 3pt off 3pt,line width=1.0pt] (-1.5,0) -- (-1.5,0.3989);
%\draw (-2.5,0.2393) node[right] {$RRH_0$};
\draw [->] (-1.9,0.0399) -- (-2.2,0.1197);
\draw (-2.15,0.1197) node[left] {$\frac{\alpha}{2}$};
\draw (-1.5,0) node[below] {$-z_{(1-\frac{\alpha}{2})}$};
\draw[fill=black,fill opacity=0.3, smooth,samples=50,domain=1.5:3.5] plot(\x,{(1/sqrt(2*pi))*exp((-(\x)^2)/2)}) -- (3.5,0) -- (1.5,0) -- cycle;
\draw [dash pattern=on 3pt off 3pt,line width=1.0pt] (1.5,0) -- (1.5,0.3989);
%\draw (1.6,0.2393) node[right] {$RRH_0$};
%\draw (-0.0,0.2393) node[] {$RNRH_0$};
\draw [->] (1.9,0.0399) -- (2.2,0.1197);
\draw (2.45,0.1197) node[left] {$\frac{\alpha}{2}$};
\draw (1.5,0) node[below] {$z _{(1-\frac{\alpha}{2})}$};
\end{tikzpicture}
%\vspace{-.6cm}
%\caption{Região crítica para o teste $Z$, para uma média, bilateral.}\label{Fig:THZRCBil}
\end{figure}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
\begin{align*}
P(-z_{(1-\frac{\alpha}{2})}<\dfrac{\sqrt{n}(T-\theta_{0})}{\sigma_{T}}<z_{(1-\frac{\alpha}{2})})&\approx 1-\alpha\\
\Rightarrow P(T-\dfrac{\sigma_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}}<\theta_{0}<T+\dfrac{\sigma_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}})&\approx 1-\alpha
\end{align*}
Notem que o intervalo
\begin{align*}
    \Big(T-\dfrac{\sigma_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}};T+\dfrac{\sigma_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}}\Big)
\end{align*}
depende de $T$ e, portanto, é um intervalo aleatório. Com isso, temos que o intervalo aleatório acima contém o valor $\theta_{0}$ com probabilidade $1-\alpha$ aproximadamente.
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Seja $t$ o valor observado de $T.$ Então, o intervalo
\begin{align*}
    \Big(t-\dfrac{\sigma_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}};t+\dfrac{\sigma_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}}\Big)\qquad (\star\star)
\end{align*}
contém ou não o valor de $\theta_{0}.$ Podemos pensar nisso como um experimento Bernoulli com probabilidade de sucesso $p,$ sendo a probabilidade do intervalo conter o valor verdadeiro. 
\end{block}
\pause
\begin{block}{}
\justifying
Em geral, a probabilidade de sucesso é, aproximadamente, $1-\alpha.$ O intervalo $(\star\star)$ é chamado de intervalo de confiança para $\theta_{0}.$ E,
\begin{itemize}
    \item $(1-\alpha)\%$ é a confiança;
    \item $\alpha$ é conhecido como nível de significância.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
De acordo com o slide anterior, podemos escrever:

\begin{align*}
    Li(t)&=t-\dfrac{\sigma_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}}\\
    Ls(t)&=t+\dfrac{\sigma_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}}
\end{align*}

$$
P_{\theta_{0}}\Big(Li(t)\leq \theta_{0}\leq Ls(t)\Big)=
\begin{cases}
0,\quad \text{se}~~\theta_{0}\notin \Big(Li(t);Ls(t)\Big)\\
1,\quad \text{se}~~\theta_{0} \in \Big(Li(t);Ls(t)\Big)
\end{cases}
$$
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Na prática não conhecemos $\sigma_{T}.$ Seja $S_{T}$ um estimador consistente para $\sigma_{T}.$ Então, temos que
\begin{align*}
    \dfrac{\sqrt{n}(T-\theta_{0})}{S_{T}}=\SetaUP{\ConvP 1}{\left(\dfrac{\sigma_{T}}{S_{T}}\right)}\SetaUP{\ConvD N(0,1)}{\dfrac{\sqrt{n}(T-\theta_{0})}{\sigma_{T}}}\SetaUP{\text{Teorema de Slutsky}}{\ConvD} N(0,1).
\end{align*}
\end{block}
\pause 
\begin{block}{}
\justifying
Com isso, obtemos que o intervalo 
\begin{align*}
    \Big(T-\dfrac{S_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}};T+\dfrac{S_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}}\Big)
\end{align*}
conterá $\theta_{0}$ com probabilidade $1-\alpha,$ aproximadamente.
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Se $t$ e $s_{t}$ são os valores observados de $T$ e $S_{T},$ respectivamente, então
\begin{align*}
    \Big(t-\dfrac{s_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}};t+\dfrac{s_{T}z_{(1-\frac{\alpha}{2})}}{\sqrt{n}}\Big)
\end{align*}
é um intervalo de confiança $(1-\alpha)\%$ aproximadamente para $\theta_{0}.$ Em que $\dfrac{s_{T}}{\sqrt{n}}$ é chamado de erro padrão de $T.$
\end{block}
\end{frame}

\begin{frame}{Exemplo: Intervalo de Confiança para Média}
\begin{block}{}
\justifying
Seja \seqX~ uma amostra aleatória de uma distribuição com média $\mu$ e variância $\sigma^{2}$ (ambos conhecidos). Seja $\Bar{X}$ e $S^{2}$ a média amostral e a variância amostral, respectivamente. Pelo TCL,
\begin{align*}
    \dfrac{\sqrt{n}(\Bar{X}-\mu)}{\sigma}\ConvD N(0,1).
\end{align*}
Além disso, $S^{2}$ é um estimador consistente para $\sigma^{2},$ então $S$ é um estimador consistente para $\sigma.$ Pelo teorema de Slutsky,
\begin{align*}
    \dfrac{\sqrt{n}(\Bar{X}-\mu)}{S}\ConvD N(0,1)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Então,
\begin{align*}
    \Big(\Bar{x}-\dfrac{1,96s}{\sqrt{n}};\Bar{x}+\dfrac{1,96s}{\sqrt{n}}\Big)
\end{align*}
é um intervalo de confiança de $95\%,$ aproximadamente, para a média $\mu,$ em que $\Bar{x}$ e $s$ são os valores observados de $\Bar{X}$ e $S.$
\end{block}
\end{frame}

\begin{frame}{Exemplo: Intervalo de Confiança para $p$}
\begin{block}{}
\justifying
Seja \seqX~ uma amostra aleatória de uma distribuição Bernoulli com parâmetro de sucesso $p\in(0,1).$ Seja $\hat{p}=\Sumi \dfrac{X_{i}}{n}$ a proporção de sucessos, considerando $P(X_{i}=1)=p$ e $P(X_{i}=0)=1-p.$ Pelo TCL,
\begin{align*}
    \sqrt{n}(\hat{p}-p)\ConvD N(0,p(1-p)),
\end{align*}
ou, equivalentemente,
\begin{align*}
    \dfrac{\sqrt{n}(\hat{p}-p)}{\sqrt{p(1-p)}}\ConvD N(0,1).
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Como $\hat{p}$ é consistente para $p,$ temos que
\begin{align*}
    \dfrac{\sqrt{n}(\hat{p}-p)}{\sqrt{\hat{p}(1-\hat{p})}}\ConvD N(0,1).
\end{align*}
Com isso, 
\begin{align*}
    \Big(\hat{p}-\dfrac{z_{(1-\frac{\alpha}{2})}\sqrt{\hat{p}(1-\hat{p})}}{\sqrt{n}};\hat{p}+\dfrac{z_{(1-\frac{\alpha}{2})}\sqrt{\hat{p}(1-\hat{p})}}{\sqrt{n}}\Big)
\end{align*}
é um intervalo de confiança $(1-\alpha)\%$ assintótico.
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Outra forma de fazer é utilizar o método delta para encontrar uma função $g$ tal que $p(1-p)[g^{'}(p)]^{2}=k~(\text{constante}).$ Segue que,
\begin{align*}
g^{'}(p)&=\dfrac{\sqrt{k}}{\sqrt{p(1-p)}},~\text{fazendo}~k=1,~\text{temos},\\
g^{'}(p)&=\dfrac{1}{\sqrt{p(1-p)}}
\end{align*}
\end{block}
\end{frame}

\begin{frame}{\Home}
\begin{block}{}
\justifying
Um intervalo de confiança $(1-\alpha)\%$ assintótico para $p$ pode ser obtido utilizando a seguinte convergência em distribuição:
\begin{align*}
    2\sqrt{n}(\arcsin{\sqrt{\hat{p}}}-\arcsin{\sqrt{p}})\ConvD N(0,1)
\end{align*}
Escreva o intervalo de confiança explicitamente!
\end{block}
\end{frame}

\begin{frame}{Exemplo: Intervalo de Confiança para $\mu$ sob Normalidade}
\begin{block}{}
\justifying
Seja \seqX~ uma amostra aleatória da distribuição $N(\mu,\sigma^{2}),$ com $\mu$ e $\sigma^{2}$ desconhecidos. Seja $\Bar{X}$ e $S^{2}$ a média e a variância amostral, respectivamente. Então,
$$T=\dfrac{\Bar{X}-\mu}{\frac{S}{\sqrt{n}}}$$
tem distribuição $t$ de student com $n-1$ graus de liberdade, ver teorema 3.6.1 do livro do Hogg, página 214 e 215 da oitava edição.
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Seja $t_{\alpha,n-1}$ o quantil $\alpha$ de uma distribuição $t$ de student com $n-1$ graus de liberdade, ou seja, $P(T<t_{\alpha,n-1})=\alpha,$ em que $T\sim t-$student$(n-1).$ Então,
\begin{align*}
    P(\Bar{x}-t_{(1-\frac{\alpha}{2})}\dfrac{s}{\sqrt{n}}<\mu<\Bar{x}+t_{(1-\frac{\alpha}{2})}\dfrac{s}{\sqrt{n}})=1-\alpha
\end{align*}
\end{block}
\end{frame}

\begin{frame}{\Home}
\begin{block}{}
\justifying

\begin{itemize}
    \item \textbf{Exercícios da seção 4.2:} 7, 8, 9, 15, 18, 21.
\end{itemize}

\end{block}
\end{frame}

\section{Intervalo de Confiança para a Diferença de Médias}
\begin{frame}{Intervalo de Confiança para a Diferença de Médias}
\begin{block}{}
\justifying
Seja $X_{1},\ldots,X_{n_{1}}$ e $Y_{1},\ldots,Y_{n_{2}}$ duas amostras aleatórias da distribuição de $X$ e $Y,$ respectivamente, em que $X$ e $Y$ são v.a.'s com $E(X)=\mu_{1},~Var(X)=\sigma_{1}^{2},~E(Y)=\mu_{2}$ e $Var(Y)=\sigma^{2}_{2}.$ Suponha independência das amostras. Estamos interessados em construir um intervalo de confiança para $\Delta=\mu_{1}-\mu_{2}.$
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Seja $\Bar{X}=\dfrac{{\displaystyle \sum_{i=1}^{n_{1}}} X_{i}}{n_{1}}$ e $\Bar{Y}=\dfrac{{\displaystyle \sum_{i=1}^{n_{2}}} Y_{i}}{n_{2}}.$ Então, $\Bar{\Delta}=\Bar{X}-\Bar{Y}$ é um estimador não viesado para $\Delta.$ Seja $n=n_{1}+n_{2},$ assuma que $\dfrac{n_{1}}{n}\xrightarrow{} \lambda_{1}>0$ e que $\dfrac{n_{2}}{n}\xrightarrow{} \lambda_{2}>0,$ com $\lambda_{1}+\lambda_{2}=1.$ Pelo Teorema Central do Limite,
\begin{align*}
\sqrt{n_{1}}(\Bar{X}-\mu_{1})\ConvD N(0,\sigma_{1}^{2}).
\end{align*}
Segue que,
\begin{align*}
\sqrt{n}(\Bar{X}-\mu_{1})=\sqrt{\dfrac{n}{n_{1}}}\sqrt{n_{1}}(\Bar{X}-\mu_{1})\ConvD N(0,\dfrac{\sigma_{1}^{2}}{\lambda_{1}}).
\end{align*}
\end{block}
\pause
\begin{block}{}
\justifying
Usamos o fato de $\sqrt{\dfrac{n}{n_{1}}}\rightLim\sqrt{\dfrac{1}{\lambda_{1}}}$ e $\sqrt{n_{1}}(\Bar{X}-\mu_{1})\ConvD N(0,\sigma_{1}^{2}).$
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Da mesma forma,
\begin{align*}
\sqrt{n}(\Bar{Y}-\mu_{2})\ConvD N(0,\dfrac{\sigma_{2}^{2}}{\lambda_{2}}).
\end{align*}
Note que,
\begin{align*}
\sqrt{n}[\Bar{X}-\Bar{Y}-(\mu_{1}-\mu_{2})]&=\sqrt{n}(\Bar{X}-\mu_{1})-\sqrt{n}(\Bar{Y}-\mu_{2})\\
&\ConvD N(0,\dfrac{\sigma_{1}^{2}}{\lambda_{1}}+\dfrac{\sigma_{2}^{2}}{\lambda_{2}})
\end{align*}
Logo,
\begin{align*}
\dfrac{\sqrt{n}(\Bar{X}-\Bar{Y}-\Delta)}{\sqrt{\dfrac{\sigma_{1}^{2}}{\lambda_{1}}+\dfrac{\sigma_{2}^{2}}{\lambda_{2}}}}\ConvD N(0,1)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
\begin{align*}
\dfrac{\sqrt{n}(\Bar{X}-\Bar{Y}-\Delta)}{\sqrt{\dfrac{\sigma_{1}^{2}}{\lambda_{1}}+\dfrac{\sigma_{2}^{2}}{\lambda_{2}}}}&\ConvD N(0,1)\\
\Rightarrow 
\dfrac{\sqrt{n}(\Bar{X}-\Bar{Y}-\Delta)}{\sqrt{\dfrac{\sigma_{1}^{2}}{\frac{n_{1}}{n}}+\dfrac{\sigma_{2}^{2}}{\frac{n_{2}}{n}}}}&=
\dfrac{(\Bar{X}-\Bar{Y}-\Delta)}{\sqrt{\dfrac{\sigma_{1}^{2}}{n_{1}}+\dfrac{\sigma_{2}^{2}}{n_{2}}}}
\ConvD N(0,1)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Substituindo $\sigma_{1}^{2}$ e $\sigma_{2}^{2}$ por $S_{1}^{2}=\dfrac{1}{n_{1}-1}\Sumi (X_{1}-\Bar{X})$ e $S_{2}^{2}=\dfrac{1}{n_{2}-1}\Sumi (Y_{1}-\Bar{Y}),$ respectivamente, temos que,
\begin{align*}
\dfrac{(\Bar{X}-\Bar{Y}-\Delta)}{\sqrt{\dfrac{S_{1}^{2}}{n_{1}}+\dfrac{S_{2}^{2}}{n_{2}}}} \ConvD N(0,1)   
\end{align*}
pois $S_{1}^{2}$ e $S_{2}^{2}$ são consistentes para $\sigma_{1}^{2}$ e $\sigma_{2}^{2},$ respectivamente.
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Logo, temos que um intervalo de confiança $(1-\alpha)100\%,$ assintótico para $\Delta$ fica dado por,
\begin{align*}
\left(\Bar{x}-\Bar{y}-z_{(1-\frac{\alpha}{2})}\sqrt{\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}};\Bar{x}-\Bar{y}+z_{(1-\frac{\alpha}{2})}\sqrt{\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}}\right)
\end{align*}
\end{block}
\end{frame}

\section{Intervalo de Confiança para a Diferença de Proporção}
\begin{frame}{}
\begin{block}{}
\justifying
Assuma as mesmas condições anteriores. Adicionalmente, suponha que $X_{i}\sim$Bernoulli$(p_{1}),~i=1,\ldots,n_{1}$ e $Y_{j}\sim$Bernoulli$(p_{2}),~j=1,\ldots,n_{2}.$ Defina $\hat{p}_{1}=\dfrac{\sum X_{i}}{n_{1}}$ e $\hat{p}_{2}=\dfrac{\sum Y_{j}}{n_{2}}.$ Segue que,
\begin{align*}
    \dfrac{\hat{p}_{1}-\hat{p}_{2}-(p_{1}-p_{2})}{\sqrt{\dfrac{p_{1}(1-p_{1})}{n_{1}}+\dfrac{p_{2}(1-p_{2})}{n_{2}}}}&\ConvD N(0,1)\\
    \SetaUP{\text{Teorema de Slutsky}}{\SeSe} 
    \dfrac{\hat{p}_{1}-\hat{p}_{2}-(p_{1}-p_{2})}{\sqrt{\dfrac{\hat{p}_{1}(1-\hat{p}_{1})}{n_{1}}+\dfrac{\hat{p}_{2}(1-\hat{p}_{2})}{n_{2}}}}&\ConvD N(0,1)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Com isso, um intervalo de confiança $(1-\alpha)100\%,$ assintótico para $p_{1}-p_{2}$ fica dado por,
\begin{align*}
    \left(\hat{p}_{1}-\hat{p}_{2}\mp z_{(1-\frac{\alpha}{2})}\sqrt{\dfrac{\hat{p}_{1}(1-\hat{p}_{1})}{n_{1}}+\dfrac{\hat{p}_{2}(1-\hat{p}_{2})}{n_{2}}}\right)
\end{align*}
\nocite{hogg}
\end{block}
\end{frame}

\begin{frame}{\Home}
\begin{block}{}
\justifying

\begin{itemize}
    \item \textbf{Exercícios da seção 4.2:} 25 ao 27.
\end{itemize}

\end{block}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{\bf Referências}
\printbibliography
\end{frame}


\end{document}
