\documentclass[12pt]{beamer}

\input{../Configuracoes/layout}


\title{Inferência Estatística II}
\author{Prof. Fernando de Souza Bastos\texorpdfstring{\\ fernando.bastos@ufv.br}{}}
\institute{Departamento de Estatística\texorpdfstring{\\ Programa de Pós-Graduação em Estatística Aplicada e Biometria}\texorpdfstring{\\ Universidade Federal de Viçosa}{}\texorpdfstring{\\ Campus UFV - Viçosa}{}}
\date{}
\newcommand\mytext{Aula 4}
\newcommand\mytextt{Fernando de Souza Bastos}
\newcommand\mytexttt{\url{https://est711.github.io/}}


\begin{document}
%\SweaveOpts{concordance=TRUE}

\frame{\titlepage}

\begin{frame}{}
\frametitle{\bf Sumário}
\tableofcontents
\end{frame}

\section{Teorema Central do Limite}
\begin{frame}{Teorema Central do Limite}
\begin{block}{}
\justifying
Seja $\{X_{n}\}_{n\geq 1}$ uma sequência de variáveis aleatórias iid com média $\mu$ e variância $\sigma^{2}<\infty.$ Então,
\begin{align*}
   \dfrac{\Sumi X_{i}-n\mu}{\sqrt{n}\sigma}=\dfrac{\sqrt{n}(\Bar{X}_{n}-\mu)}{\sigma} \ConvD N(0,1)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Demonstração}
\begin{block}{}
\justifying
Assuma, sem perda de generalidade, $\mu=0$ e $\sigma^{2}=1.$
\begin{align*}
    M_{\frac{\sum X_{i}-n\mu}{\sqrt{n}\sigma}}(t)&=M_{\frac{\sum X_{i}}{\sqrt{n}}}(t)
    =E\Big(e^{\frac{t\sum X_{i}}{\sqrt{n}}}\Big)=E\Big(\Prodi e^{\frac{tX_{i}}{\sqrt{n}}}\Big)\\
    &=\Prodi E\Big(e^{\frac{tX_{i}}{\sqrt{n}}}\Big)=\Prodi M_{X_{i}}\big(\frac{t}{\sqrt{n}}\big)=\Big(M_{X_{i}}(\dfrac{t}{\sqrt{n}})\Big)^{n}\\
\end{align*}
Ou seja, $\ln{M_{\frac{\sum X_{i}}{\sqrt{n}}}(t)}=n\ln{M_{X_{i}}(\frac{t}{\sqrt{n}})}=\dfrac{\ln{M_{X_{i}}(\frac{t}{\sqrt{n}})}}{\frac{1}{n}},$ aplicando L'Hôpital, temos:
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
\begin{align*}
    \Lim \frac{\ln{M_{X_{i}}(\frac{t}{\sqrt{n}})}}{\frac{1}{n}}&=\Lim \dfrac{\frac{M_{X_{i}}^{'}(\frac{t}{\sqrt{n}})}{M_{X_{i}}(\frac{t}{\sqrt{n}})}t(-\frac{1}{2})n^{-\frac{3}{2}}}{-\frac{1}{n^{2}}}\\
    &=\dfrac{t}{2}\Lim \sqrt{n}M_{X_{i}}^{'}(\frac{t}{\sqrt{n}})\\
    &=\dfrac{t}{2}\Lim \dfrac{M_{X_{i}}^{'}(\frac{t}{\sqrt{n}})}{\frac{1}{\sqrt{n}}}\\
    &=\dfrac{t}{2}\Lim \dfrac{M_{X_{i}}^{''}(\frac{t}{\sqrt{n}})t(-1/2)n^{-3/2}}{-\frac{1}{2}n^{-3/2}}\\
    &=\dfrac{t^{2}}{2}\Lim M_{X_{i}}^{''}(\frac{t}{\sqrt{n}})=\dfrac{t^{2}}{2}
\end{align*}
Logo, $\dfrac{\sum X_{i}}{\sqrt{n}}\ConvD X\sim N(0,1)$
\end{block}
\end{frame}

\section{Método Delta}
\begin{frame}{Método Delta}
\begin{block}{}
\justifying
Suponha que conhecemos a distribuição de uma variável aleatória, mas que queremos determinar a distribuição de uma função dela. Isso também é verdade na teoria assintótica, o teorema de Slutsky's e o teorema visto em aula, imediatamente anterior a ele, são ilustrações disso. Outro resultado desse tipo é chamado de método delta. Vejamos o próximo slide!
\end{block}
\end{frame}

%\begin{frame}{}
%\begin{Teorema}
%\justifying   
%Suponha que g(x) seja diferenciável em x. Então podemos escrever
%$$g(y) = g(x) + g'(x)(y - x) + o(|y - x|).$$ Em que, a notação $o$ significa que $a = o(b)$ se e somente se $\frac{a}{b} \rightarrow 0$, conforme $b \rightarrow 0$.
%\end{Teorema}
%\end{frame}

\begin{frame}{Método Delta}
\begin{Teorema}
\justifying
Seja $\{X_{n}\}_{n\geq 1}$ uma sequência de variáveis aleatórias satisfazendo a seguinte convergência em distribuição:
$$\sqrt{n}(X_{n}-\theta)\ConvD N(0,\sigma^{2}).$$ Se $g$ é uma função diferenciável em $\theta$ e $g'(\theta)\neq 0,$ então $$\sqrt{n}(g(X_{n})-g(\theta))\ConvD N(0, [g'(\theta)]^{2}\sigma^{2}).$$
\end{Teorema}
\end{frame}

\begin{frame}{Demonstração}
\begin{block}{}
\justifying
Vamos mostrar primeiro que $X_{n}\ConvP \theta.$ Seja $\varepsilon>0$ e $m>0$ inteiro ($m\in \N^{*}$) fixado. 
\begin{align*}
    P(|X_{n}-\theta|<\varepsilon)&=P(|\sqrt{n}(X_{n}-\theta)|<\varepsilon\sqrt{n})\\
    &\SetaUP{\text{para}~n\geq \Big(\dfrac{m}{\varepsilon}\Big)^{2}}{\geq} P(|\sqrt{n}(X_{n}-\theta)|<m)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Continuação da Demonstração}
\vspace{-0.2cm}
\begin{block}{}
\justifying
Segue que, 
\begin{align*}
    \LimInf P(|X_{n}-\theta|<\varepsilon)&\geq \LimInf P(|\sqrt{n}(X_{n}-\theta)|<m)\\
    &=\Lim P(|\sqrt{n}(X_{n}-\theta)|<m)\\
    &\SetaUP{\text{Usando o fato de que}\\ \sqrt{n}(X_{n}-\theta)\ConvD N(0,\sigma^{2})}{=}P(|Z|<\sigma m),~Z\sim N(0,1)
\end{align*}
\end{block}
\pause
\vspace{-0.2cm}
\begin{block}{}
\justifying
Assim,
\begin{align*}
    \LimInf P(|X_{n}-\theta|<\varepsilon)&\geq P(|Z|<\sigma m)=\Phi(\sigma m)-\Phi(-\sigma m)\\
    &=2\Phi(\sigma m)-1
\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Em resumo,
\begin{align*}
    \LimInf P(|X_{n}-\theta|<\varepsilon)&\geq 2\Phi(\sigma m)-1,~\forall m\in \mathbb{Z}^{*}\\  
    \LimInf P(|X_{n}-\theta|<\varepsilon)&\geq \lim_{m\rightarrow +\infty} \Big[2\Phi(\sigma m)-1\Big]\\
    &=1
\end{align*}
\end{block}
\pause
\begin{block}{}
\justifying
Como $\limsup\geq \liminf,$ temos que
\begin{align}\label{num1}
    \limsup P(|X_{n}-\theta|<\varepsilon)=1\Rightarrow \Lim P(|X_{n}-\theta|<\varepsilon)
\end{align}
\end{block}
\pause
\begin{block}{}
\justifying
Expandindo $g$ em série de Taylor até a primeira ordem em torno de $\theta,$ temos que $$g(x)=g(\theta)+g'(\theta)(x-\theta)(x-\theta)+\mathcal{C}(x)(x-\theta),$$ em que ${\displaystyle \lim_{x\rightarrow \theta} \mathcal{C}(x)=0}.$
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Segue que, 
\begin{align*}
g(x)-g(\theta)&=g'(\theta)(x-\theta)(x-\theta)+\mathcal{C}(x)(x-\theta)\\
\sqrt{n}\big(g(X_{n})-g(\theta)\big)&=\sqrt{n}(X_{n}-\theta)\big(g'(\theta)-\mathcal{C}(X_{n}\big).
\end{align*}
Para mostrar o resultado desejado, devemos mostrar que $\mathcal{C}(X_{n})\ConvP 0.$ Com isso, utilizando o teorema de Slutsky, o resultado é obtido. Como ${\displaystyle \lim_{x\rightarrow \theta} \mathcal{C}(x)=0},~\text{para}~\varepsilon>0, \exists~\delta>0$ tal que $|x-\theta|<\delta\Rightarrow~|\mathcal{C}(x)|<\varepsilon.$
\end{block}
\pause
\begin{block}{}
\justifying
Daí,
\begin{align*}
    \{|X_{n}-\theta|<\delta\}&\subset \{|\mathcal{C}(X_{n})|<\varepsilon\}\Rightarrow P(|X_{n}-\theta|<\delta)\leq P(|\mathcal{C}(X_{n})|<\varepsilon)
\end{align*}
Esse resultado, juntamente com $(\ref{num1})$ implica que,
\begin{align*}
    \Lim P(|\mathcal{C}(X_{n})|<\varepsilon)=1,~\text{ou seja},~\mathcal{C}(X_{n})\ConvP 0.
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Exercício}
\begin{block}{\Home}
\justifying
Seja $X_{n}\sim Binomial(n,p).$ Mostre que 
\begin{align*}
    \sqrt{n}\big(\arcsin{\sqrt{\dfrac{X_{n}}{n}}}-\arcsin{\sqrt{p}}\big)\ConvD N\big(0,\dfrac{1}{4}\big)
\end{align*}
\end{block}
\nocite{casella2021statistical} \nocite{hogg}
\end{frame}

\begin{frame}{}
\begin{block}{\Home}
\justifying
Exercícios 5.3.1 à 5.3.8, 5.3.11, 5.3.12
\end{block}
\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{\bf Referências}
\printbibliography
\end{frame}


\end{document}
