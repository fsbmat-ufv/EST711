\documentclass[12pt]{beamer}

\input{../Configuracoes/layout}
\setLayoutColor{2} 

\title{Inferência Estatística II}
\author{Prof. Fernando de Souza Bastos\texorpdfstring{\\ fernando.bastos@ufv.br}{}}
\institute{Departamento de Estatística\texorpdfstring{\\ Programa de Pós-Graduação em Estatística Aplicada e Biometria}\texorpdfstring{\\ Universidade Federal de Viçosa}{}\texorpdfstring{\\ Campus UFV - Viçosa}{}}
\date{}
\newcommand\mytext{Aula 14}
\newcommand\mytextt{Fernando de Souza Bastos}
\newcommand\mytexttt{\url{https://est711.github.io/}}
\begin{document}
%\SweaveOpts{concordance=TRUE}

\frame{\titlepage}

\begin{frame}{}
\frametitle{\bf Sumário}
\tableofcontents
\end{frame}

\section{Razão de Verossimilhança Monótona}
\begin{frame}{Razão de Verossimilhança Monótona}
\begin{block}{}
	\justifying
	A razão de verossimilhança é definida como o quociente 
	
	\[
	\frac{L(\theta_0 | T(X))}{L(\theta_1 | T(X))}
	\]
	
	para dois valores de parâmetro \(\theta_0\) e \(\theta_1\). Dizemos que a razão de verossimilhança é \textbf{monótona} se, para quaisquer dois valores \(\theta_1 > \theta_0\), a razão de verossimilhança for uma função \textbf{monótona não-decrescente} ou \textbf{não-crescente} em relação à estatística \(T(X)\). Ambas as formas permitem a construção de testes uniformemente mais poderosos.
\end{block}

%\begin{block}{}
%	\justifying
%	Suponha que tenhamos dois modelos estatísticos, \(M_0\) e \(M_1\), com funções de verossimilhança associadas \(L(\theta_0 | T(X))\) e \(L(\theta_1 | T(X))\), respectivamente, onde \(X\) representa uma amostra observada e \(T(X)\) é uma estatística específica resumindo as informações contidas na amostra. A Razão de Verossimilhança Monótona compara essas duas verossimilhanças de uma maneira especial.
%\end{block}
\end{frame}

\begin{frame}{Razão de Verossimilhança Monótona}
%\begin{block}{}
%\justifying
%A razão de verossimilhança é definida como o quociente 
%
%\[
%\frac{L(\theta_0 | T(X))}{L(\theta_1 | T(X))}
%\]
%
%para dois valores de parâmetro \(\theta_1\) e \(\theta_0\). Dizemos que a razão de verossimilhança é \textbf{monótona} se, para quaisquer dois valores \(\theta_1 > \theta_0\), a razão de verossimilhança for uma função \textbf{monótona não-decrescente} em relação a estatística \(T(X)\).
%\end{block}
%\pause
\begin{block}{}
	\justifying
	A monotonicidade da razão de verossimilhança é uma propriedade crucial, pois assegura que, se um modelo é mais plausível do que outro para um certo valor da estatística \(T(X)\), essa relação de preferência se mantém à medida que \(T(X)\) aumenta (ou diminui, dependendo do caso). Isso permite estabelecer uma ordenação consistente entre os modelos, o que é essencial na construção de testes de hipóteses mais poderosos e na tomada de decisões inferenciais.
\end{block}

\end{frame}

%\begin{frame}{Razão de Verossimilhança Monótona}
%\begin{definicao}
%\justifying
%Uma família de funções de densidade de probabilidade \(\{f_{\vec{X}}(\cdot; \theta), \theta \in \Theta\}\), \(\Theta \in \mathbb{R}\), é dita ter Razão de Verossimilhança Monótona (RVM) se existir uma estatística \(T = t(\vec{X})\) tal que, para todo \(\theta_1 > \theta_0\),
%
%\[
%\frac{L(\theta_0 \mid T(\vec{X}))}{L(\theta_1 \mid T(\vec{X}))}
%\]
%
%é uma função monótona (não decrescente ou não crescente) em \(t(\vec{x})\), para \(\vec{x} \in \{f(\vec{x}; \theta_0) > 0 \text{e} f(\vec{x}; \theta_1) > 0\}\), em que \(f_{\vec{X}}(\vec{x}; \theta_0) \neq f_{\vec{X}}(\vec{x}; \theta_1)\).
%
%\end{definicao}
%\end{frame}

\begin{frame}{Razão de Verossimilhança Monótona}
	\begin{definicao}
		\justifying
		Uma família de densidades \(\{f_{\vec{X}}(\cdot; \theta), \theta \in \Theta\}\), com \(\Theta \subseteq \mathbb{R}\), possui a \textbf{propriedade da razão de verossimilhança monótona (RVM)} se existe uma estatística \(T = t(\vec{X})\) tal que, para quaisquer \(\theta_1 > \theta_0\), a razão
		
		\[
		\frac{L(\theta_0 \mid T(\vec{X}))}{L(\theta_1 \mid T(\vec{X}))}
		\]
		
		é uma função monótona (não decrescente ou não crescente) em \(t(\vec{x})\), para todo \(\vec{x}\) tal que \(f_{\vec{X}}(\vec{x}; \theta_0) > 0\), \(f_{\vec{X}}(\vec{x}; \theta_1) > 0\), e \(f_{\vec{X}}(\vec{x}; \theta_0) \neq f_{\vec{X}}(\vec{x}; \theta_1)\).
	\end{definicao}
\end{frame}


\subsection{Exemplo 1}
\begin{frame}{Exemplo 1: Distribuição Exponencial}
	\vspace{-0.3cm}
	\begin{block}{}
		\justifying
		Seja \(X_1, \ldots, X_n\) uma amostra aleatória de \(X \sim \text{Exp}(\frac{1}{\theta})\), com densidade
		
		\[
		f(x;\theta) = \frac{1}{\theta} \exp\left(-\frac{x}{\theta}\right), \quad \theta > 0.
		\]
		
		A função de verossimilhança para a amostra \(\vec{X}\) é:
		
		\[
		L(\theta \mid \vec{x}) = \left(\frac{1}{\theta}\right)^n \exp\left(-\frac{1}{\theta} \sum_{i=1}^n x_i\right).
		\]
	\end{block}
\end{frame}

\begin{frame}{Exemplo 1: Distribuição Exponencial}
	\begin{block}{}
		\justifying
		Assim, a razão de verossimilhança para dois valores \(\theta_0\) e \(\theta_1\) é:
		
		\[
		\frac{L(\theta_0 \mid \vec{x})}{L(\theta_1 \mid \vec{x})}
		= \frac{\theta_1^n}{\theta_0^n} \exp\left(-\left(\frac{1}{\theta_1} - \frac{1}{\theta_0} \right) \sum_{i=1}^{n} x_i\right).
		\]
		
		Para \(\theta_1 > \theta_0\), a razão acima é uma função \textbf{monótona não decrescente} em \(t(\vec{x}) = \sum x_i\). Portanto, a distribuição \(\text{Exp}(\theta)\) possui \textbf{RVM crescente} em \(t(\vec{x}) = \sum x_i\). Alternativamente, também possui \textbf{RVM crescente} em \(t^*(\vec{x}) = -\sum x_i\).
	\end{block}
\end{frame}

%\begin{frame}{Exemplo 1}
%\vspace{-0.3cm}
%\begin{block}{}
%\justifying
%Seja $X_1, \ldots, X_n$ uma amostra de $X \sim \text{Exp}(\theta)$. Segue que,
%\[
%\frac{L(\theta_0 | T(\tend{X}))}{L(\theta_1 | T(\tend{X}))} = \frac{\theta_{1}^{n}}{\theta_{0}^{n}} \exp\left(-\left(\frac{1}{\theta_1} - \frac{1}{\theta_0}\right)\sum_{i=1}^{n} x_i\right)
%\]
%\end{block}
%\pause
%\vspace{-0.3cm}
%\begin{block}{}
%\justifying
%Para $\theta_{1}>\theta_{0}$ fixados, a função acima é monótona não crescente em $t(\Vec{x}) = \Sumi x_i,$ logo, $\text{Exp}(\theta)$ tem Razão de Verossimilhança Monótona (RVM) não crescente em $t(\Vec{x}) = \Sumi x_i$. De outro lado, $\text{Exp}(\theta)$ tem RVM não decrescente em $t^{*}(\Vec{x}) = -\Sumi x_i$.
%\end{block}
%\end{frame}

\begin{frame}{Exemplo 2}
\begin{block}{}
\justifying
Seja \(X_1, \ldots, X_n\) uma amostra de \(X \sim U(0, \theta)\). Considere \(y_{n} = \max\{X_1, \ldots, X_n\}\),

\[
L(\vec{x}; \theta) =\frac{1}{\theta^n} 1_{(0, \theta)}(x_(n))= L(\vec{y}; \theta)= \frac{1}{\theta^n} 1_{(0, \theta)}(y_n)
\]

Assim, se \(\theta_0 < \theta_1\), temos que:

\[
\frac{L(\vec{y}; \theta_0)}{L(\vec{y}; \theta_1)} = \left(\frac{\theta_1}{\theta_0}\right)^n \frac{1_{(0, \theta_0)}(y_n)}{1_{(0, \theta_1)}(y_n)}
\]

\end{block}
\end{frame}

\begin{frame}{Exemplo 2}
	\begin{block}{}
		\justifying
		Portanto, a distribuição \(U(0, \theta)\) possui Razão de Verossimilhança Monótona (RVM) \textbf{não crescente} em \(y_n = \max\{X_1, \ldots, X_n\}\), pois:
		
		\[
		g(y_n) = \begin{cases} 
			\left(\frac{\theta_0}{\theta_1}\right)^n, & \text{se } 0 < y_n < \theta_0 \\
			0, & \text{se } \theta_0 \leq y_n < \theta_1 \\
			\text{Indeterminado}, & \text{se } y_n \geq \theta_1
		\end{cases}
		\]
	\end{block}
\end{frame}


\section{Teorema de Karlin-Rubin}
\begin{frame}{Teorema de Karlin-Rubin}
\vspace{-0.2cm}
\begin{block}{}
\justifying
Considere testar $H_{0}: \theta \leq \theta_0$ versus $H_{1}: \theta > \theta_0.$
Seja $L(\theta; \Vec{x})$ a função de verosimilhança de uma distribuição com espaço de parâmetros $\Theta$ e estatística suficiente $T = t(\Vec{x})$ para $\theta,$ tal que, para quaisquer $\theta_0$ e $\theta_1$ em $\Theta$ tais que $\theta_0 < \theta_1$, a razão de verosimilhança
\begin{align*}
\Lambda(\Vec{x}; \theta_0, \theta_1) &= \frac{L(\theta_0; \Vec{x})}{L(\theta_1; \Vec{x})}
\end{align*}
é uma RVM (Ou seja, é uma função monótona (não decrescente ou não crescente) em $t(x)$). Sob tais condições, para um apropriado valor $t_{0},$ 
\begin{align*}
    C=\{\Vec{x};T(\Vec{x})> t_{0}\} 
\end{align*}
é uma região crítica para um teste uniformemente mais poderoso para $H_{0}: \theta \leq \theta_0$ versus $H_{1}: \theta > \theta_0$ de tamanho $\alpha=P_{\theta_{0}}(T(\Vec{x})> t_{0})$.
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{Observação:}
\justifying
Sob as condições do teorema anterior podemos testar $H_{0}: \theta \geq \theta_0$ versus $H_{1}: \theta < \theta_0.$ Nesse caso, 
\begin{align*}
    C=\{\Vec{x};T(\Vec{x})< t_{0}\} 
\end{align*}
é uma região crítica para um teste uniformemente mais poderoso para $H_{0}: \theta \geq \theta_0$ versus $H_{1}: \theta < \theta_0$ de tamanho $\alpha=P_{\theta_{0}}(T(\Vec{x})< t_{0})$.
\end{block}
\end{frame}

\subsection{Exemplo 1}
\begin{frame}{Exemplo 1}
\begin{block}{}
\justifying
Considere \seqX $\sim$ Poisson$(\theta).$ Encontre um TUMP de nível $\alpha$ para testar 
\begin{align*}
    H_{0}:\theta\leq \theta_{0}~\text{contra}~H_{1}:\theta> \theta_{0}
\end{align*}
\end{block}
\pause
\begin{block}{}
	\justifying
	Considere $0<\theta_{0}<\theta_{1}<\infty$ e
	\begin{align*}
		\dfrac{L(t;\theta_{0})}{L(t;\theta_{1})}=e^{n(\theta_{1}-\theta_{0})}\left(\dfrac{\theta_{0}}{\theta_{1}}\right)^{t},~\text{com}~t=T(\Vec{X})=\Sumi X_{i}.
	\end{align*}
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
	\justifying
	Sabemos que $T(\Vec{X})=\Sumi X_{i}$ é uma estatística suficiente para $\theta,$ como a razão anterior é uma RVM de $t,$ para $\theta_{0}<\theta_{1},$ temos que, pelo teorema de Karlin-Rubin, o teste que rejeita $H_{0}$ para toda amostra $\Vec{X}$ tal que $\Sumi X_{i}>t_{0}$ é um teste uniformemente mais poderoso de tamanho $\alpha=P_{\theta_{0}}\left(\Sumi X_{i}>t_{0}\right).$ 
\end{block}
\end{frame}

%\begin{frame}{}
%\begin{block}{}
%\justifying
%Logo, a família Poisson tem a propriedade de RVM. Pelo teorema de Karlin-Rubin, o teste que rejeita $H_{0}$ para toda amostra $\Vec{X}$ tal que $\Sumi X_{i}>t_{0}$ é um teste uniformemente mais poderoso de tamanho $\alpha=P_{\theta_{0}}(\Sumi X_{i}>t_{0}).$
%\end{block}
%\end{frame}

\subsection{Exemplo 2}

\begin{frame}{Exemplo 2}
	\begin{block}{}
		\justifying
		Suponha o problema de teste:
		\begin{align*}
			H_{0}&:\theta \leq 1 \quad \text{versus} \quad H_{1}: \theta > 1\\
			n &= 100\\
			\alpha &= 0{,}05
		\end{align*}
		
		Sabemos que, sob \(X_i \sim \text{Poisson}(\theta)\), a estatística \(T = \sum_{i=1}^{n} X_i\) tem distribuição \(T \sim \text{Poisson}(n\theta)\). Assim, sob \(H_0\), a distribuição mais favorável à \(H_0\) é quando \(\theta = 1\), e portanto:
		
		\[
		T = \sum X_i \sim \text{Poisson}(100).
		\]
	\end{block}
\end{frame}
\begin{frame}{Continuação}
	\begin{block}{}
		\justifying
		Utilizando o software \textsf{R}, obtemos que o menor valor inteiro \(t_0\) tal que:
		
		\[
		P_{\theta = 1}(T > t_0) \leq 0{,}05
		\]
		
		é \(t_0 = 117\), pois:
		
		\[
		P_{\theta = 1}(T > 117) = P(T \geq 118) \approx 0{,}043.
		\]
		
		Logo, o teste que rejeita \(H_0\) sempre que \(\sum X_i > 117\) é um TUMP de nível \(\alpha = 0{,}05\). O tamanho exato do teste é \(P_{\theta = 1}(T \geq 118) \approx 0{,}043\).
	\end{block}
\end{frame}


\subsection{Exemplo 3}
\begin{frame}{Exemplo 3}
\begin{block}{}
\justifying
Seja \seqX~ uma amostra aleatória de uma distribuição exponencial$(\theta).$ Encontre um TUMP de 
\begin{align*}
    H_{0}:\theta\geq \theta_{0}~\text{contra}~H_{1}:\theta< \theta_{0}
\end{align*}
\end{block}
\pause
\begin{block}{}
\justifying
Sabemos que $T(\Vec{X})=\Sumi X_{i}$ é uma estatística suficiente para $\theta$ e que a distribuição de $T(\Vec{X})=\Sumi X_{i}$ é Gamma$(n,\theta).$ Para $0<\theta_{0}<\theta_{1}<\infty$
\begin{align*}
    \dfrac{L(t;\theta_{0})}{L(t;\theta_{1})}=\left(\dfrac{\theta_{1}}{\theta_{2}}\right)^{n}e^{t\left(\frac{1}{\theta_{1}}-\frac{1}{\theta_{0}}\right)},~\frac{1}{\theta_{1}}-\frac{1}{\theta_{0}}<0, 
\end{align*}
é uma RVM de $t.$ Logo, a família gamma tem a propriedade RVM. 
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
\justifying
Pelo teorema de Karlin-Rubin, o teste que rejeita $H_{0}$ para toda amostra $\Vec{X}$ tal que $\Sumi X_{i}<3$ é um teste uniformemente mais poderoso de tamanho $\alpha=P_{\theta_{0}}(\Sumi X_{i}<t_{0}).$
\end{block}
\pause
\begin{block}{}
\justifying
Considere, por exemplo, 
\begin{align*}
    H_{0}&:\theta\geq 3~\text{contra}~H_{1}:\theta< 3\\
    n&=5\\
    \alpha&=0.05
\end{align*}
Segue que $\Sumi X_{i}\sim~$Gamma$(5,\theta),$ sob $H_{0}, \Sumi X_{i}\sim~$Gamma$(5,3).$
\end{block}
\end{frame}

\begin{frame}{}
\begin{block}{}
    No software R, obtemos $t_{0}=3,05$ e $P_{\theta_{0}}(\Sumi X_{i}>t_{0})\approx 0,05.$ Logo, o teste que rejeita $H_{0}$ para toda amostra de tamanho $n=5$ quando $\Sumi X_{i}<3,05$ é um TUMP de tamanho $0.05.$ 
\end{block}
\nocite{hogg, casella2021statistical}
\end{frame}

%\begin{frame}{\Home}
%\begin{block}{}
%\justifying

%\begin{itemize}
%    \item \textbf{Exercícios da seção 8.2:} 1,3,5,6,11,13.
%\end{itemize}
%\end{block}
%
%\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{\bf Referências}
\printbibliography
\end{frame}


\end{document}

