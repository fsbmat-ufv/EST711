---
title: "Verossimilhanças"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(shiny)
library(patchwork)  # para mostrar dois gráficos lado a lado
knitr::opts_chunk$set(echo = FALSE)
```

## Estimador de Máxima Verossimilhança para \( X_i \sim \text{Bernoulli}(\theta) \)

Dada uma amostra \( X_1, X_2, \dots, X_n \), em que \( X_i \sim \text{Bernoulli}(\theta) \), a função de verossimilhança é definida como:

\[
L(\theta) = \prod_{i=1}^{n} P(X_i = x_i) = \prod_{i=1}^{n} \theta^{x_i} (1 - \theta)^{1 - x_i}.
\]

Simplificando a expressão, temos:

\[
L(\theta) = \theta^{\sum_{i=1}^{n} x_i} (1 - \theta)^{n - \sum_{i=1}^{n} x_i}.
\]

Aqui, \( \sum_{i=1}^{n} x_i \) é a soma dos sucessos (ou seja, a soma das observações em que \( X_i = 1 \)).

### Log-verossimilhança

Para facilitar o cálculo do estimador de máxima verossimilhança, tomamos o logaritmo natural da função de verossimilhança, o que nos dá a log-verossimilhança:

\[
\ell(\theta) = \log L(\theta) = \left( \sum_{i=1}^{n} x_i \right) \log(\theta) + \left( n - \sum_{i=1}^{n} x_i \right) \log(1 - \theta).
\]

### Maximização da Log-verossimilhança

Para encontrar o estimador de máxima verossimilhança, derivamos a log-verossimilhança em relação a \( \theta \) e igualamos a zero:

\[
\frac{d\ell(\theta)}{d\theta} = \frac{\sum_{i=1}^{n} x_i}{\theta} - \frac{n - \sum_{i=1}^{n} x_i}{1 - \theta} = 0.
\]

Multiplicando ambos os lados por \( \theta (1 - \theta) \), obtemos:

\[
\sum_{i=1}^{n} x_i (1 - \theta) = (n - \sum_{i=1}^{n} x_i) \theta.
\]

Agora, expandimos e reagrupamos os termos para isolar \( \theta \):

\[
\sum_{i=1}^{n} x_i = n \theta.
\]

Finalmente, resolvemos para \( \theta \):

\[
\hat{\theta} = \frac{\sum_{i=1}^{n} x_i}{n}.
\]

Portanto, o estimador de máxima verossimilhança para \( \theta \) é a média amostral:

\[
\hat{\theta} = \frac{\sum_{i=1}^{n} x_i}{n}.
\]

É fácil verificar que $\hat{p}=\dfrac{y}{n},$ em que $y=\sum_{i=1}^{n} x_i$ é máximo global, pois se $y=0$ ou $y=n,$ temos
\begin{align*}
\log{L(p|\boldsymbol{x})}=
\begin{cases}
      n\log{(1-p)},&\text{se}~y=0\\
      n\log{p},&\text{se}~y=n
    \end{cases}\,.
\end{align*}
Em qualquer um dos casos temos $\log{L(p|\boldsymbol{x})}$ é uma função monótona de $p$ e, portanto, $\hat{p}=\dfrac{y}{n}$ é ponto de máximo. Assim, $\hat{p}=\dfrac{y}{n}$ é o EMV de $p.$


```{r Bernoulli, exercise=TRUE}
# Função de verossimilhança para dados binários (Bernoulli)
verossimilhanca <- function(theta, x) {
  n <- length(x)                  # Número de observações
  soma_x <- sum(x)                # Soma dos sucessos (x_i = 1)
  return(theta^soma_x * (1 - theta)^(n - soma_x))
}

# Dados de exemplo: uma amostra binária com 1s e 0s
numeros <- c(0,1)
x <- sample(numeros, 10, replace = TRUE) # Exemplo de uma amostra

# Calculando o estimador de máxima verossimilhança (EMV)
EMV <- mean(x)  # Média amostral é o EMV para Bernoulli

# Definindo o intervalo de theta
theta_vals <- seq(0, 1, length.out = 100)

# Calculando a verossimilhança para cada valor de theta
L_theta <- sapply(theta_vals, verossimilhanca, x = x)

# Criando o dataframe para o ggplot
df <- data.frame(theta = theta_vals, L_theta = L_theta)

# Criando o gráfico com a linha do EMV
ggplot(df, aes(x = theta, y = L_theta)) +
  geom_line(color = "blue", size = 1.2) +  # Linha da verossimilhança
  geom_vline(xintercept = EMV, linetype = "dashed", color = "red", size = 1) +  # Linha vertical no EMV
  labs(
    title = "Função de Verossimilhança Bernoulli com EMV",
    x = expression(theta),
    y = expression(L(theta))
  ) +
  ylim(0,0.005)+
  annotate("text", x = EMV, y = max(L_theta), label = paste("EMV =", round(EMV, 2)), 
           vjust = -1, color = "red", size = 5) +  # Texto anotando o valor do EMV
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )

```

```{r Bernoulli2, exercise=TRUE}
# Carregar bibliotecas
library(ggplot2)

# Definir número de amostras
n <- 10  # você pode ajustar para um valor maior
p_vals <- seq(0, 1, length.out = 100)  # valores de p de 0 a 1

# Verossimilhança para o caso y = n (todos os x_i = 1)
y_n <- n  # y = n
log_likelihood_y_n <- n * log(p_vals)  # função de log-verossimilhança para y = n

# Verossimilhança para o caso y = 0 (todos os x_i = 0)
y_0 <- 0  # y = 0
log_likelihood_y_0 <- n * log(1 - p_vals)  # função de log-verossimilhança para y = 0

# Criar gráfico para y = n
plot_y_n <- ggplot(data.frame(p_vals, log_likelihood_y_n), aes(x = p_vals, y = log_likelihood_y_n)) +
  geom_line(color = "blue", size = 1) +
  labs(title = expression(paste("Log-Verossimilhança para ", y == n)),
       x = expression(p),
       y = "Log-Verossimilhança") +
  theme_minimal() +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red") + # Estimador de máxima verossimilhança p = 1
  annotate("text", x = 1, y = max(log_likelihood_y_n), label = "p = 1", vjust = -1, color = "red")

# Criar gráfico para y = 0
plot_y_0 <- ggplot(data.frame(p_vals, log_likelihood_y_0), aes(x = p_vals, y = log_likelihood_y_0)) +
  geom_line(color = "blue", size = 1) +
  labs(title = expression(paste("Log-Verossimilhança para ", y == 0)),
       x = expression(p),
       y = "Log-Verossimilhança") +
  theme_minimal() +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") + # Estimador de máxima verossimilhança p = 0
  annotate("text", x = 0, y = max(log_likelihood_y_0), label = "p = 0", vjust = -1, color = "red")

# Mostrar os gráficos
plot_y_n
plot_y_0

```

- **Gráfico 1:** Para \( y = n \) (todos \( x_i = 1 \)), a log-verossimilhança \( \ell(p|\boldsymbol{x}) = n \log(p) \) é uma função crescente de \( p \), com máximo em \( p = 1 \). A linha vermelha indica o valor \( p = 1 \), que é o EMV.

- **Gráfico 2:** Para \( y = 0 \) (todos \( x_i = 0 \)), a log-verossimilhança \( \ell(p|\boldsymbol{x}) = n \log(1 - p) \) é uma função decrescente de \( p \), com máximo em \( p = 0 \). A linha vermelha indica o valor \( p = 0 \), que é o EMV.


## Distribuição Exponencial: Cálculo do Estimador de Máxima Verossimilhança


Dada uma amostra \( X_1, X_2, \ldots, X_n \sim \text{Exp}(\theta) \), a função de densidade da exponencial é dada por:
\[
f(x \mid \theta) = \frac{1}{\theta} e^{-\frac{x}{\theta}}, \quad x > 0, \theta > 0.
\]

A função de verossimilhança \( L(\theta) \) para \( n \) observações \( X_1, X_2, \ldots, X_n \) é o produto das densidades individuais:
\[
L(\theta) = \prod_{i=1}^{n} \left( \frac{1}{\theta} e^{-\frac{X_i}{\theta}} \right).
\]
Simplificando, temos:
\[
L(\theta) = \frac{1}{\theta^n} \exp\left( -\frac{1}{\theta} \sum_{i=1}^{n} X_i \right).
\]

Para facilitar a maximização, tomamos o logaritmo da verossimilhança, obtendo a \textbf{log-verossimilhança}:
\[
\ell(\theta) = \log L(\theta) = -n \log \theta - \frac{1}{\theta} \sum_{i=1}^{n} X_i.
\]

### Maximização da Log-Verossimilhança

Para encontrar o estimador de máxima verossimilhança (EMV), derivamos a função de log-verossimilhança \( \ell(\theta) \) em relação a \( \theta \) e igualamos a zero:
\[
\frac{\partial \ell(\theta)}{\partial \theta} = -\frac{n}{\theta} + \frac{1}{\theta^2} \sum_{i=1}^{n} X_i = 0.
\]

Multiplicando ambos os lados por \( \theta^2 \) para simplificar:
\[
-n \theta + \sum_{i=1}^{n} X_i = 0.
\]

Isolando \( \theta \), obtemos:
\[
\hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} X_i.
\]

Assim, o estimador de máxima verossimilhança \( \hat{\theta} \) é a \textbf{média amostral}:
\[
\hat{\theta} = \bar{X}.
\]

O estimador de máxima verossimilhança para o parâmetro \( \theta \) na distribuição exponencial é dado pela \textbf{média amostral} \( \bar{X} \), o que significa que a média das observações é o valor que maximiza a função de log-verossimilhança.



```{r Exponencial, exercise=TRUE, exercise.eval=TRUE}
# Função de log-verossimilhança correta (com o sinal negativo)
log_verossimilhanca <- function(theta, x) {
  n <- length(x)                # Número de observações
  soma_x <- sum(x)              # Soma das observações
  return(-n * log(theta) - soma_x / theta)
}

# Gerando dados de exemplo: uma amostra exponencial
#set.seed(123)
x <- rexp(10, rate = 1/2)  # Exemplo de uma amostra com taxa 1/2 (ou seja, theta = 2)

# Definindo o intervalo de theta
theta_vals <- seq(0.1, 10, length.out = 100)

# Calculando a log-verossimilhança para cada valor de theta
log_L_theta <- sapply(theta_vals, log_verossimilhanca, x = x)

# Calculando o estimador de máxima verossimilhança (EMV)
EMV <- mean(x)  # Média amostral é o EMV para Exponencial

# Criando o dataframe para o ggplot
df <- data.frame(theta = theta_vals, log_L_theta = log_L_theta)

# Criando o gráfico com a linha do EMV
ggplot(df, aes(x = theta, y = log_L_theta)) +
  geom_line(color = "blue", size = 1.2) +  # Linha da log-verossimilhança
  geom_vline(xintercept = EMV, linetype = "dashed", color = "red", size = 1) +  # Linha vertical no EMV
  labs(
    title = "Função de Log-Verossimilhança para a Distribuição Exponencial",
    x = expression(theta),
    y = expression(ell(theta))
  ) +
  annotate("text", x = EMV, y = max(log_L_theta), label = paste("EMV =", round(EMV, 2)), 
           vjust = 1, color = "red", size = 5) +  # Texto anotando o valor do EMV
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12)
  )

```

## Cálculo do Estimador de Máxima Verossimilhança (EMV) para \(\theta\) na Distribuição Uniforme

Dada uma amostra \( X_1, X_2, \ldots, X_n \sim U(0, \theta] \), a função de densidade da uniforme é dada por:
\[
f(x \mid \theta) = \frac{1}{\theta}, \quad x \in (0, \theta], \quad \theta > 0.
\]

A função de verossimilhança \( L(\theta) \) para \( n \) observações \( X_1, X_2, \ldots, X_n \) é o produto das densidades individuais:
\[
L(\theta) = \prod_{i=1}^{n} f(x_i \mid \theta) = \prod_{i=1}^{n} \frac{1}{\theta} \cdot \delta(0, \theta],
\]
onde
\[
\delta(0, \theta] = 
\begin{cases}
1, & \text{se} \ x_i \in (0, \theta] \ \forall \ i, \\
0, & \text{caso contrário}.
\end{cases}
\]

Simplificando, temos:
\[
L(\theta) = \left( \frac{1}{\theta} \right)^n \delta\{ \max(x_1, \ldots, x_n) \leq \theta \}.
\]
Note que a função de verossimilhança é \( L(\theta) = 0 \) se \( \theta < \max(x_1, \ldots, x_n) \), pois pelo suporte da distribuição uniforme, todos os \( x_i \) devem estar no intervalo \( (0, \theta] \).

### Maximização da Verossimilhança

Para maximizar a verossimilhança, a função \( L(\theta) \) será maximizada quando \( \theta \) for o menor valor possível que satisfaça \( \theta \geq \max(x_1, \ldots, x_n) \). Ou seja, a verossimilhança é maximizada quando:
\[
\hat{\theta} = \max(X_1, \ldots, X_n).
\]

O estimador de máxima verossimilhança para o parâmetro \( \theta \) na distribuição uniforme \( U(0, \theta] \) é dado pelo valor máximo das observações, ou seja:
\[
\hat{\theta} = \max(X_1, \ldots, X_n).
\]

```{r Uniforme, exercise=TRUE, exercise.eval=TRUE}
# Definir os parâmetros
#set.seed(123)  # Para reprodutibilidade
n <- 10  # Número de amostras
x <- runif(n, min = 0, max = 5)  # Amostras de uma distribuição uniforme

# Estimador de máxima verossimilhança
theta_hat <- max(x)

# Função de verossimilhança
L_theta <- function(theta) {
  if (theta > max(x)) {
    return((1/theta)^n)  # Verossimilhança se theta > max(x)
  } else {
    return(0)  # Verossimilhança é 0 se theta <= max(x)
  }
}

# Valores de theta
theta_values <- seq(0, 10, length.out = 100)
likelihood_values <- sapply(theta_values, L_theta)

# Criar um dataframe para o ggplot
df <- data.frame(theta = theta_values, likelihood = likelihood_values)

# Criar o gráfico
ggplot(df, aes(x = theta, y = likelihood)) +
  geom_line(color = "blue") +
  geom_vline(xintercept = theta_hat, linetype = "dashed", color = "red") +
  labs(title = "Função de Verossimilhança para U(0, θ]",
       x = "θ",
       y = "L(θ)") +
  theme_minimal() +
  annotate("text", x = theta_hat + 0.2, y = max(likelihood_values) * 0.8,
           label = paste("Estimador: ", round(theta_hat, 2)),
           color = "red")
```


## Verossimilhança da Distribuição Normal

Sejam \( X_1, X_2, \ldots, X_n \sim N(\theta, 1) \) iid e \( L(\theta|\boldsymbol{x}) \) denota a função de verossimilhança. Então, temos:

\begin{align*}
    L(\theta|\boldsymbol{x}) &= \prod_{i=1}^{n} f(x_i \mid \theta) \\
    &= \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(x_{i} - \theta)^{2}} \\
    &= \frac{1}{(2\pi)^{\frac{n}{2}}} e^{-\frac{1}{2} \sum_{i=1}^{n} (x_{i} - \theta)^{2}}.
\end{align*}

Para encontrar o estimador de máxima verossimilhança, consideramos a log-verossimilhança:

\begin{align*}
    \ell(\theta|\boldsymbol{x}) &= \log L(\theta|\boldsymbol{x}) \\
    &= -\frac{n}{2} \log(2\pi) - \frac{1}{2} \sum_{i=1}^{n} (x_{i} - \theta)^{2}.
\end{align*}

Agora, para maximizar a log-verossimilhança, derivamos em relação a \( \theta \):

\begin{align*}
    \frac{d}{d\theta} \ell(\theta|\boldsymbol{x}) &= -\frac{1}{2} \frac{d}{d\theta} \sum_{i=1}^{n} (x_{i} - \theta)^{2} \\
    &= -\frac{1}{2} \sum_{i=1}^{n} \frac{d}{d\theta} (x_{i} - \theta)^{2} \\
    &= -\frac{1}{2} \sum_{i=1}^{n} 2(x_{i} - \theta)(-1) \\
    &= \sum_{i=1}^{n} (x_{i} - \theta).
\end{align*}

Igualando a derivada a zero, obtemos:

\begin{align*}
    \sum_{i=1}^{n} (x_{i} - \theta) = 0.
\end{align*}

Isso se simplifica para:

\begin{align*}
    \sum_{i=1}^{n} x_{i} = n\theta,
\end{align*}

e, portanto, temos que:

\begin{align*}
    \hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} x_{i} = \bar{x}.
\end{align*}

Assim, o estimador de máxima verossimilhança é dado por \( \hat{\theta} = \bar{x} \).



```{r Normal, exercise=TRUE, exercise.eval=TRUE}
# Carregar as bibliotecas necessárias
library(ggplot2)

# Definir os parâmetros
#set.seed(123)  # Para reprodutibilidade
n <- 10  # Número de amostras
x <- rnorm(n, mean = 5, sd = 1)  # Amostras de uma distribuição normal

# Estimador de máxima verossimilhança
theta_hat <- mean(x)

# Função de verossimilhança
L_theta <- function(theta) {
  n <- length(x)
  return((1 / (sqrt(2 * pi)))^n * exp(-0.5 * sum((x - theta)^2)))
}

# Valores de theta
theta_values <- seq(2, 8, length.out = 100)
likelihood_values <- sapply(theta_values, L_theta)

# Criar um dataframe para o ggplot
df <- data.frame(theta = theta_values, likelihood = likelihood_values)

# Criar o gráfico
ggplot(df, aes(x = theta, y = likelihood)) +
  geom_line(color = "blue") +
  geom_vline(xintercept = theta_hat, linetype = "dashed", color = "red") +
  labs(title = "Função de Verossimilhança para N(θ, 1)",
       x = "θ",
       y = "L(θ)") +
  theme_minimal() +
  annotate("text", x = theta_hat + 0.1, y = max(likelihood_values) * 0.8,
           label = paste("Estimador: ", round(theta_hat, 2)),
           color = "red")

```

## Principio da Invariância


### Objetivo

Queremos visualizar a igualdade

$$
L_{\vec X_n}^{g}(\hat\eta) \;=\; L_{\vec X_n}(\hat\theta),
$$

onde $L(\theta,\vec X_n)=\prod_{i=1}^n f(X_i;\theta)$ é a função de verossimilhança (até constante em $\theta$), $\eta=g(\theta)$ é uma reparametrização (possivelmente não injetiva) e

$$
L_{\vec X_n}^{g}(\eta) \;=\; \max_{\theta \in A(\eta)} L_{\vec X_n}(\theta),\qquad
A(\eta)=\{\theta\in\Theta: g(\theta)=\eta\}.
$$

Graficamente:

1. Plotamos $L(\theta)$ em função de $\theta$ e identificamos $\hat\theta$.
2. Projetamos $\hat\theta$ para $\hat\eta=g(\hat\theta)$.
3. Construímos $L^g(\eta)$ tomando, para cada $\eta$, o **máximo** de $L(\theta)$ ao longo da fibra $A(\eta)$.
4. O pico de $L^g$ em $\hat\eta$ tem a **mesma altura** do pico de $L$ em $\hat\theta$.

# Exemplo base (Normal com variância conhecida e $g(\theta)=\theta^2$)

Neste exemplo, $X_i \sim \mathcal N(\theta,\sigma^2)$ com $\sigma$ conhecida. O EMV é $\hat\theta=\bar X$. Escolhemos $g(\theta)=\theta^2$, **não injetiva**, então

$$
L^g(\eta)=\max\{L(+\sqrt{\eta}),\, L(-\sqrt{\eta})\}.
$$


```{r simulate-data}
set.seed(1)
n        <- 30
sigma    <- 1
theta_0  <- 0.8             # valor verdadeiro para simulação
x        <- rnorm(n, mean = theta_0, sd = sigma)

# log-verossimilhança (até constante em theta)
ll_theta <- function(theta) {
  sum(dnorm(x, mean = theta, sd = sigma, log = TRUE))
}

theta_hat <- mean(x)        # EMV em theta
eta_hat   <- theta_hat^2    # imagem do EMV: g(hat{theta})

# Grades para os gráficos
theta_grid <- seq(min(-2, theta_hat)-1, max(2, theta_hat)+1, length.out = 1200)
ll_vals    <- sapply(theta_grid, ll_theta)

eta_grid   <- seq(0, max(theta_grid^2), length.out = 800)
ll_g_vals  <- sapply(eta_grid, function(eta) {
  t1 <-  sqrt(eta); t2 <- -sqrt(eta)
  max(ll_theta(t1), ll_theta(t2))
})

# Normalizações para comparar picos (opcional, apenas visual)
ll_vals_n   <- ll_vals   - max(ll_vals)
ll_g_vals_n <- ll_g_vals - max(ll_g_vals)
```

```{r plot-both, fig.cap="À esquerda: log-verossimilhança em θ. À direita: log-verossimilhança induzida em η=g(θ)=θ² (envelope máximo sobre as fibras). Os picos têm a mesma altura: L^g(\\hat{η}) = L(\\hat{θ})."}
p_theta <- data.frame(theta = theta_grid, ll = ll_vals_n) |>
  ggplot(aes(theta, ll)) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = theta_hat, linetype = 2) +
  geom_point(aes(x = theta_hat, y = 0), size = 2) +
  annotate("text", x = theta_hat, y = 0.12,
           label = expression(hat(theta)), vjust = -0.5, parse = TRUE) +
  labs(title = "Verossimilhança em θ",
       x = expression(theta), y = "log L(θ) (normalizada)")

p_eta <- data.frame(eta = eta_grid, ll = ll_g_vals_n) |>
  ggplot(aes(eta, ll)) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = eta_hat, linetype = 2) +
  geom_point(aes(x = eta_hat, y = 0), size = 2) +
  annotate("text", x = eta_hat, y = 0.12,
           label = expression(hat(eta)==g(hat(theta))), vjust = -0.5, parse = TRUE) +
  labs(title = "Verossimilhança induzida em η = g(θ) = θ²",
       x = expression(eta), y = expression(log~L^g(eta)~(normalizada)))

p_theta + p_eta
```

## Leitura do gráfico

* **Esquerda**: o pico em $\theta=\hat\theta$ é o máximo de $L(\theta)$.
* **Direita**: para cada $\eta$, tomamos $L(\sqrt\eta)$ e $L(-\sqrt\eta)$ e ficamos com o **máximo** — isto é o *envelope* $L^g(\eta)$.
* Na abscissa $\eta=\hat\eta=g(\hat\theta)$, a ordenada coincide com a do pico em $\hat\theta$. Assim, $L^g(\hat\eta)=L(\hat\theta)$.

# Vendo as fibras $A(\eta)=g^{-1}\{\eta\}$

Para um $\eta$ fixo, a fibra é $A(\eta)=\{\sqrt{\eta},-\sqrt{\eta}\}$. O valor de $L^g(\eta)$ é a **maior** altura de $L(\theta)$ nesses dois pontos.

```{r fibers, fig.height=4.8, fig.cap="Construção de L^g(η) como envelope máximo em uma fibra {−√η, +√η}."}
eta_demo <- eta_hat  # escolha uma eta próxima do pico
A        <- c(-sqrt(eta_demo), +sqrt(eta_demo))

df_theta <- data.frame(theta = theta_grid, ll = ll_vals)
df_pts   <- data.frame(theta = A, ll = sapply(A, ll_theta))

ggplot(df_theta, aes(theta, ll)) +
  geom_line(linewidth = 1) +
  geom_point(data = df_pts, aes(theta, ll), size = 2) +
  geom_segment(aes(x = A[1], xend = A[2],
                   y = max(df_pts$ll), yend = max(df_pts$ll)),
               linetype = 3) +
  annotate("text", x = mean(A), y = max(df_pts$ll),
           vjust = -0.8, label = expression(L^g(eta)==max~L(theta)~colon~theta%in%A(eta)),
           parse = TRUE) +
  labs(title = "Fibra A(η) e construção do envelope",
       x = expression(theta), y = "log L(θ)")
```

# Caso injetivo e monótono (comparação)

Se $g$ é injetora e monótona (por ex., $g(\theta)=\theta$ ou $g(\theta)=\exp(\theta)$ com $\Theta=\mathbb R$), então cada $\eta$ corresponde a um único $\theta=g^{-1}(\eta)$, e

$$
L^g(\eta)=L\big(g^{-1}(\eta)\big).
$$

O máximo é o mesmo ponto “reparametrizado”, e a **altura do pico** também é a mesma.

```{r injective-case, fig.cap="Reparametrização injetiva e monótona: o pico é preservado ao compor com g^{-1}."}
g_inj      <- function(theta) exp(theta)
ginv_inj   <- function(eta) log(eta)

eta_grid_2 <- seq(min(g_inj(theta_grid[1]), 1e-3), max(g_inj(tail(theta_grid,1))), length.out = 800)
ll_g2_vals <- sapply(eta_grid_2, function(eta) {
  theta <- ginv_inj(eta)
  ll_theta(theta)
})

# Normalizar para comparar visualmente
ll_g2_vals_n <- ll_g2_vals - max(ll_g2_vals)

p_theta2 <- data.frame(theta = theta_grid, ll = ll_vals_n) |>
  ggplot(aes(theta, ll)) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = theta_hat, linetype = 2) +
  labs(title = "Verossimilhança em θ (caso base)",
       x = expression(theta), y = "log L(θ) (normalizada)")

p_eta2 <- data.frame(eta = eta_grid_2, ll = ll_g2_vals_n) |>
  ggplot(aes(eta, ll)) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = g_inj(theta_hat), linetype = 2) +
  labs(title = "Verossimilhança induzida em η = g(θ) = e^θ",
       x = expression(eta), y = expression(log~L^g(eta)~(normalizada)))

p_theta2 + p_eta2
```

# Resumo conceitual

* **Definição**: $L^g(\eta)=\max_{\theta\in A(\eta)} L(\theta)$.
* **No máximo**: como $\hat\eta=g(\hat\theta)$ e $\hat\theta\in A(\hat\eta)$,

  $$
  L^g(\hat\eta)\;\ge\;L(\hat\theta).
  $$

  Por outro lado, como $L(\hat\theta)$ é o **máximo global** de $L$,

  $$
  L^g(\hat\eta)\;\le\;\max_{\theta} L(\theta) \;=\; L(\hat\theta).
  $$

  Logo, $L^g(\hat\eta)=L(\hat\theta)$.
* **Intuição gráfica**: $L^g$ é o “invólucro superior” obtido varrendo as fibras $A(\eta)$. O topo permanece com a **mesma altura**.

# (Opcional) Parâmetros do exemplo

Você pode ajustar abaixo e *Knitar* novamente.

```{r parametros-rapidos, eval=FALSE}
n        <- 50       # tamanho amostral
sigma    <- 0.8      # desvio conhecido
theta_0  <- -0.5     # valor verdadeiro
g        <- function(theta) theta^2
```


```{r invariancia, exercise=TRUE, exercise.eval=TRUE}
## ===== Parâmetros que você pode alterar =====
g_choice <- "square"   # "square" (g(theta)=theta^2) ou "exp" (g(theta)=exp(theta))
eta      <- 0.8        # eta >= 0 para "square"; eta > 0 para "exp"

## ===== Modelo e dados (Normal com sigma conhecida) =====
set.seed(1)
n       <- 30
sigma   <- 1
theta0  <- 0.8
x       <- rnorm(n, mean = theta0, sd = sigma)

## Log-verossimilhanca (ate constante em theta)
ll_theta <- function(theta) sum(dnorm(x, mean = theta, sd = sigma, log = TRUE))

## EMV em theta
theta_hat <- mean(x)

## Grade de theta e valores de log L(theta)
theta_grid <- seq(-3.5, 3.5, length.out = 1200)
ll_vals    <- vapply(theta_grid, ll_theta, numeric(1))

## ===== Defina g, a preimagem A(eta) e L^g(eta) =====
if (g_choice == "square") {
  if (eta < 0) stop("Para g(theta)=theta^2, use eta >= 0.")
  A_eta <- if (abs(eta) < 1e-12) 0 else c(-sqrt(eta), +sqrt(eta))
  eta_grid <- seq(0, max(theta_grid^2), length.out = 800)
  ll_g <- vapply(eta_grid, function(e) {
    t1 <-  sqrt(e); t2 <- -sqrt(e)
    max(ll_theta(t1), ll_theta(t2))
  }, numeric(1))
  eta_hat <- theta_hat^2
} else if (g_choice == "exp") {
  if (eta <= 0) stop("Para g(theta)=exp(theta), use eta > 0.")
  A_eta <- log(eta)
  eta_grid <- seq(exp(min(theta_grid)), exp(max(theta_grid)), length.out = 800)
  ll_g <- vapply(eta_grid, function(e) ll_theta(log(e)), numeric(1))
  eta_hat <- exp(theta_hat)
} else {
  stop("g_choice desconhecida; use 'square' ou 'exp'.")
}

## ===== Graficos (base R) =====
op <- par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))
on.exit(par(op), add = TRUE)

## (1) log L(theta) com a fibra A(eta) destacada
plot(theta_grid, ll_vals, type = "l", lwd = 2,
     xlab = "theta", ylab = "log L(theta)",
     main = "log-likelihood em theta")
abline(v = theta_hat, lty = 2)
if (length(A_eta) > 0 && all(is.finite(A_eta))) {
  pts_y <- vapply(A_eta, ll_theta, numeric(1))
  points(A_eta, pts_y, pch = 19)
}

## (2) log L^g(eta) com linhas em eta e eta_hat
plot(eta_grid, ll_g, type = "l", lwd = 2,
     xlab = "eta", ylab = "log L^g(eta)",
     main = "log-likelihood induzida em eta")
abline(v = eta, lty = 3)      # eta escolhido
abline(v = eta_hat, lty = 2)  # eta_hat = g(theta_hat)
```



<!--**Dica**: para usar outros modelos, basta substituir a função `ll_theta()` pela log-verossimilhança apropriada (por exemplo, Bernoulli com $p$ e $g(p)=\text{logit}(p)$; Poisson com $\lambda$ e $g(\lambda)=\sqrt{\lambda}$). O procedimento gráfico é idêntico: construir $L(\theta)$, definir $A(\eta)$ e tomar o máximo ao longo da fibra para obter $L^g(\eta)$.-->

